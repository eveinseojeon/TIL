# 6. 학습 관련 기술들

---

- 신경망(딥러닝) 학습의 효율과 정확도를 높일 수 있는 기법들을 알아 보자.



## 6.1. 매개변수 갱신

- 신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것인데 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 **최적화**(optimization)라 한다. <u>최적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화하기 좋다.</u>

### 6.1.2. 확률적 경사 하강법(SGD)

- 매개변수의 기울기(미분)를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복해서 점점 최적의 값에 다가가는 방법
- <u>지금 있는 장소에서 가장 크게 기울어진 방향으로 일정 거리만큼 가자!</u>
- SGD의 단점: 비등방성(anisotropy) 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이다. ->  최솟값까지 지그재그로 이동
- 또한 SGD가 지그재그로 탐색하는 근본 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리켜서이다.
- SGD의 이러한 단점을 개선해 주는 대체 기법 아래 세 가지

### 6.1.4. 모멘텀(Momentum)

- 모멘텀은 '운동량'을 뜻하는 단어로, 물리와 관계가 있다.
- 



### 6.1.5. AdaGrad



### 6.1.6. Adam

















## 6.2. 가중치의 초깃값











## 6.3. 배치 정규화









## 6.4. 바른 학습을 위해

- 오버피팅
- 오버피팅 억제 기술
  - 가중치 감소
  - 드롭아웃
  - 









## 6.5. 적절한 하이퍼파라미터 값 찾기



























