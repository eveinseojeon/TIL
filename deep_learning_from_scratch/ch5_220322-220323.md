# 5. 오차역전파법

---

- 4장에서 신경망 학습에 대해 설명할 때, 신경망의 가중치 매개변수에 대한 손실 함수의 기울기를 수치 미분을 사용해서 구했다. 그러나 수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점이다.

- 가중치 매개변수의 기울기를 효율적으로 계산하는 **오차역전파법**(backpropagation)을 배워 보자.
- 오차역전파법을 제대로 이해하는 방법은 1) 수식을 통한 것, 2) 계산 그래프를 통한 것 두 가지가 있다. 이번 장에서는 2) <u>계산 그래프를 사용해서 '시각적'으로 이해해 보자.</u>



## 5.1. 계산 그래프

- **순전파**(forward propagation): 계산을 왼쪽에서 오른쪽으로 진행 (출발점 -> 종착점)
- **역전파**(backward propagation): 계산을 오른쪽에서 왼쪽으로 진행 (출발점 <- 종착점)
- 계산 그래프의 장점
  1. 국소적 계산에 집중한다. 즉 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있다.
  2. 중간 계산 결과를 모두 보관할 수 있다.
  3. 역전파를 통해 각 변수의 '미분'을 효율적으로 계산할 수 있다.



## 5.2. 연쇄법칙

- 역전파는 '국소적인 미분'을 오른쪽에서 왼쪽으로 전달하는 것인데, 이 전달 원리는 **연쇄법칙**(chain rule)에 따른 것이다.
- **합성 함수**: 여러 함수로 구성된 함수
- 연쇄법칙의 원리: <u>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</u>
- 역전파가 하는 일이 연쇄법칙의 원리와 같다!



## 5.3. 역전파

- 덧셈 노드의 역전파는 입력 값을 그대로 다음 노드로 흘려보낸다. (순방향 입력 신호의 값이 필요하지 않음)
- 곱셈 노드의 역전파는 입력 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 다음 노드로 흘려보낸다. (순방향 입력 신호의 값이 필요하므로 변수에 저장해 둠)
- 이제 <u>신경망의 처리 과정을 계층이라는 단위로 구현할 것이다.</u> 모든 계층에서 forward와 backward라는 메서드를 구현한다. forward는 데이터를 순방향으로 전파하고 backward는 역방향으로 전파함으로써 <u>가중치 매개변수의 기울기를 효율적으로 구할 수 있다.</u> 이처럼 동작을 계층으로 모듈화한 덕분에, 신경망의 계층을 자유롭게 조합하여 원하는 신경망을 쉽게 만들 수 있다.



## 5.5. 활성화 함수 계층 구현하기

- ReLU 계층
- Sigmoid 계층
  - y = Sigmoid(x)일 때 Sigmoid 함수의 미분값은 y(1-y)이다.



## 5.6. Affine/Softmax 계층 구현하기

- Affine 계층

  - X*W : 선형 변환

  - X*W + B : Affine 변환(신경망의 순전파 때 수행하는 행렬의 곱)

- Softmax-with-Loss 계층

  - 순전파 (아래는 3클래스 분류 가정했을 때)

    - y<sub>1</sub> + y<sub>2</sub> + y<sub>3</sub> = 1 (probability)
    - t<sub>1</sub> + t<sub>2</sub> + t<sub>3</sub> = 1 (one-hot encoding)

    |               | Softmax |               |               | **Cross Entropy Error** |            |
    | ------------- | ------- | ------------- | ------------- | ----------------------- | ---------- |
    | **input**     |         | **output**    |               |                         |            |
    |               |         | **input**     | **input**     |                         | **output** |
    | (score)       |         | (probability) | (true)        |                         | (loss)     |
    | a<sub>1</sub> | ->      | y<sub>1</sub> | t<sub>1</sub> | ->                      |            |
    | a<sub>2</sub> | ->      | y<sub>2</sub> | t<sub>2</sub> | ->                      | L          |
    | a<sub>3</sub> | ->      | y<sub>3</sub> | t<sub>3</sub> | ->                      |            |

  - 역전파

    - 역전파 결과: Softmax 계층의 출력(y<sub>1</sub>, y<sub>2</sub>, y<sub>3</sub>)과 정답 레이블(t<sub>1</sub>, t<sub>2</sub>, t<sub>3</sub>)의 차분
    - 이렇듯 결과가 둘의 오차로 말끔히 떨어지는 것은 CEE 함수가 그렇게 설계되었기 때문이다.
    - 코드로 구현할 때는 역전파 때 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파해야 한다! (주의)

    |                               | **Softmax-with-Loss** |           |
    | ----------------------------- | --------------------- | --------- |
    | **output**                    |                       | **input** |
    | y<sub>1</sub> - t<sub>1</sub> | <-                    |           |
    | y<sub>2</sub> - t<sub>2</sub> | <-                    | ∂L/∂L = 1 |
    | y<sub>3</sub> - t<sub>3</sub> | <-                    |           |



## 5.7. 오차역전파법 구현하기

- 신경망 학습의 순서

  0. 전제

     : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.

  1. 미니배치

     : 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표.

  2. 기울기 산출

     : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.

  3. 매개변수 갱신

     : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.

  4. 반복

     : 1~3단계를 반복한다.

  - 이 중에서 오차역전파법이 등장하는 단계는 '2. 기울기 산출'이다. <u>오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있다!</u>

- 오차역전파법을 적용한 신경망을 구현할 때는 신경망의 계층을 OrderedDict(순서가 있는 딕셔너리)에 보관하는 점이 중요하다. -> 순전파 때는 추가한 순서대로 각 계층을 호출, 역전파 때는 반대 순서로 호출.
- 기울기를 구하는 방법은 두 가지가 있다. 하나는 수치 미분을 써서 구하는 방법, 또 하나는 해석적으로 수식을 풀어 구하는 방법인데, 후자인 해석적 방법은 오차역전파법을 이용하여 매개변수가 많아도 효율적으로 계산할 수 있다. 느린 수치 미분 대신 오차역전파법을 사용할 것이지만, <u>수치 미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요하다.</u> 수치 미분은 구현하기 쉽기 때문에 수치 미분의 구현에는 버그가 숨어 있기 어려운 반면, 오차역전파법은 구현하기 복잡해서 종종 실수가 있다. 그래서 <u>수치 미분의 결과와 오차역전파법의 결과를 비교하여 오차역전파법을 제대로 구현했는지 검증</u>하곤 하는데, 이처럼 두 방식으로 구한 기울기가 일치함(엄밀히 말하면 거의 같음)을 확인하는 작업을 **기울기 확인**(gradient check)이라고 한다.



---

## 메모

- 기계학습 문제는 회귀(regression)와 분류(classification)로 나뉘는데, 각각에 대응하는 손실 함수를 사용하면 역전파 결과가 출력값(y<sub>n</sub>)과 정답 레이블(t<sub>n</sub>)의 차분, 즉 둘의 오차(y<sub>n</sub>-t<sub>n</sub>)로 말끔히 떨어진다. 각 손실 함수가 그렇게 설계되었기 때문이다.

|          | 출력층 함수     | 손실 함수               |
| -------- | --------------- | ----------------------- |
| **회귀** | 항등 함수       | 오차제곱합(SSE)         |
| **분류** | 소프트맥스 함수 | 교차 엔트로피 오차(CEE) |


