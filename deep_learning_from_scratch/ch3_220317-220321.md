# 3. 신경망

---



## 3.2. 활성화 함수(activation function)

- Step 함수
- Sigmoid 함수
- Tanh 함수
- ReLU 함수
- Leaky ReLU 함수
- ELU 함수



## 3.5. 출력층 설계하기

- Sigmoid 함수와 Logit 함수

  - odds: 특정 사상 X의 승산. 즉 특정 사상 X에 대하여, 배타적인 다른 사상이 일어날 확률 대비 사상 X가 일어날 확률
  - Logit 함수: odds에 밑이 e인 log 취하기
    - 클래스가 2개인 경우: Sigmoid 함수와 역함수 관계
    - 클래스가 n개(3개 이상)인 경우: Softmax 함수와 역함수 관계
- Softmax 함수

  - Sigmoid 함수의 일반형
  - 출력층에서 다중분류를 위해 사용하는 함수
  - 기계학습 중 학습 단계(의 출력층)에서 사용하고, 추론 단계(의 출력층)에서는 생략하는 것이 일반적이다.
  - Softmax(a<sub>k</sub>) = y<sub>k</sub>
  - a<sub>k</sub>(-∞ ~ ∞ 범위의 score)를 받아서 y<sub>k</sub>(0 ~ 1 범위의 probability)로 바꿔준다. 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력하는 것이다.
  - y<sub>1</sub> +  y<sub>2</sub> + y<sub>3</sub> + ... + y<sub>n</sub> = 1
  - a<sub>k</sub>는 input data가 k 클래스로 분류될 score를 의미하는데, Logit으로 가정되어 있기 때문에 Softmax 함수에서 e(자연상수)를 사용한다. 이 a<sub>k</sub>가 Softmax 함수의 input으로 들어가고, 아웃풋으로 나오는 y<sub>k</sub>는 결국 input data가 k 클래스로 분류될 확률값을 의미함.
  - 오버플로 문제에 대한 대책으로 분자와 분모 양쪽에서 입력 신호 중 최댓값을 빼준다.



## 3.6. 손글씨 숫자 인식

- pickle 모듈: 객체를 바이너리 파일로 저장
- axis=0 : 열   /   axis=1 : 행

