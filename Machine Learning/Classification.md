# Classification



## 로지스틱 회귀(Logistic Regression)

: 지도학습 중 분류 알고리즘

y가 범주형일 때 사용



이항 로지스틱 회귀 모형

: 반응변수 y의 범주가 2가지인 경우. 관심범주와 관심이 아닌 범주



Logit 함수(로그오즈)

logit(p) = log(p/(1-p))



0~1 사이의 확률값 p에 대하여,

오즈 = p / (1-p)

오즈는 0~ 무한대 사이의 실수값

p가 커지면 오즈도 커진다.

그것에 로그를 취하면 전 실구간으로 확대됨.



즉, 0~1 사이의 값 p를 전 실구간 범위로 변환한 것.



p=1/2 이면 오즈는 1이 된다.

-> 관심사건이 발생할 확률, 발생하지 않을 확률이 똑같은 상태.



파라미터 추정에 사용하는 방법

: 최대우도추정법, 경사하강법



로지스틱 회귀모형은 선형의 분리 경계면을 갖는다.

X라는 특성변수들만의 공간에서 Y를 기준짓는 선형의 경계

따라서 선형 분리가 가능한 문제만 해결할 수 있다.



시그모이드 함수는 x의 지점마다 기울기가 다 다르기 때문에

계수를 해석할 때 오즈비 개념을 활용한다.

x가 1만큼 증가하면, 성공에 대한 오즈가 exp(β)만큼 변화한다.



## 나이브 베이즈(Naive Bayes)

: 지도 학습 가운데 분류 알고리즘

나이브 베이즈 분류기는 생성 모델(generative model)임

스팸 메일 분류에 자주 활용됨



각 특성변수들이 모두 독립이라고 가정 (그래서 이름이 naive)



조건부 확률(베이즈 정리)를 이용



장점

- 데이터의 크기가 커도 연산 속도가 빠름
- 학습에 필요한 데이터 양이 적어도 좋은 성능을 보이는 편
- 다양한 텍스트 분류나 추천 등에 활용됨



단점

- Zero frequency 문제(-> 모든 빈도에 1을 더해주는 라플라스 스무딩으로 보정하여 극복)나 Underflow(컴퓨터가 계산하기에 너무 작은 숫자) 문제가 있음
- 모든 독립변수가 독립이라는 가정이 너무 단순함



## KNN(K-nearest Neigbor) 분류

: 가장 간단한 지도학습 알고리즘

훈련 데이터를 저장해두는 것이 모델을 만드는 과정의 전부임

K개의 가장 가까운 이웃 중에서 어떤 범주가 가장 비중이 높은가?



새로운 자료가 오면 가장 거리가 가까운 k개를 찾는다

k개 중 개수가 많은 라벨로 분류



K의 결정

: KNN에서 K의 결정은 매우 중요하다.

K가 작으면 이상치 등의 노이즈에 민감하게 반응 -> 오버피팅 문제 (모델 복잡, 분산 높)

K가 크면 자료의 패턴을 잘 파악할 수 없어 예측 성능이 저하됨 -> 언더피팅 문제 (모델 단순, 편향 높)

-> 하이퍼파라미터 K를 찾기 위하여 검증 데이터를 이용하여 주어진 훈련 데이터에 가장 적절한 K를 찾아야 함



거리의 측정법

- 유클리디안 거리 : 차이 제곱의 합 (물리적 거리)
- 맨해튼 거리 : 차이 절대값의 합
- 민코우스키 거리 : 차이의 절대값의 p제곱의 합의 1/p 제곱 (수학적 거리. 일반식. p=1이면 맨해튼, p=2이면 유클리디안)



거리를 측정할 때 자료의 스케일의 차이가 있는 경우,

스케일이 큰 특성변수에 의해 거리가 결정되어 버릴 수 있음.

따라서 각 특성변수별로 스케일이 유사해지도록 표준화 변환(Z score) 또는 min-max 변환으로 스케일링을 해준 뒤에 거리를 재는 것이 적절하다.



min-max 변환

: xi-min(xi) / max(xi)-min(xi)

-> min(xi)=0, max(xi)=1이 되며

크기 순서대로 정렬하여 0~1 사이 값으로 표현됨.


