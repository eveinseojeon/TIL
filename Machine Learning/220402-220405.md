## 머신러닝의 기본 개념 및 방법론의 분류

머신러닝



머신러닝 - 지도학습, 비지도학습, 강화학습



지도학습 - X, Y가 둘 다 있음

비지도학습 - X만 있음



지도학습 (Supervised Learning)

: x에 대한 y라벨이 주어져 있는 훈련용 데이터에서 x와 y의 관계를 표현하는 함수를 찾아 목표변수인 라벨을 예측하도로 모델을 학습.

y라벨의 데이터 타입에 따라 회귀(regression), 분류(classification)로 나뉨.

- 회귀 : y라벨이 연속형. x에 따른 y의 예측값을 찾는 직선 찾기
- 분류 : y라벨이 범주형. y값들을 분리하는 경계 찾기



대표 알고리즘

- 회귀 : Linear regression
- 분류 : k-nearest neighbors, Logistic regression, softmax regression, Naive Bayesian
- 회귀, 분류 모두 가능 : decision tree, SVM, Random forest, Boosting, Neural Network, Deep learning





비지도학습 (Unsupervised Learning)

: y라벨이 없는 훈련용 데이터에서 x들만 모아놓고 특징 변수들 간의 관계나 유사성을 기반으로 의미있는 패턴을 학습, 추출. 자율학습이라고도 함.

군집화(clustering), 차원축소(dimension reduction), 추천 시스템(recommendation) 등에 활용됨. 

- 군집화 : 특징 변수들의 유사성을 기준으로 없던 라벨을 불여주기
- 차원축소 : 특징 변수들이 너무 많을 때는 예측 모형을 만들려고 해도 모델링이 잘 안 되고, 데이터의 패턴을 찾기도 힘들다. 이 때 특징 변수들만 모아서 차원 축소를 한다. 차원축소의 기본 원리는 모든 특징 변수들이 가진 정보를 최대한 반영하되 소수의 특징 변수만 남기겠다는 것. 해석을 풍부하게 하고 예측력을 높일 수 있다. 지도학습에서 y라벨이 있을 때도, 특징 변수들과 y라벨의 2차원 시각화를 위해 차원축소를 사용할 수도 있음.
- 추천 시스템



대표 알고리즘

- 군집화 : k-means clustering, hierarchical clustering
- 차원축소 : PCA(principal component analysis: 주성분 분석), LDA, t-SNE
- 추천 시스템 : Apriori

Auto-Encoders





강화학습

: 행동하는 주체(agent)가 있고 어떤 정책(policy)에 따라 행동(action)을 했을 때의 상태(state)와 보상(reward)을 바꿔주는 환경(environment)으로 구성됨. 주체가 매번 어떤 행동을 하면 환경에 의해 상태와 보상이 바뀌면서 주체는 보상이 가장 커지는 방향으로 계속 학습해 나가게 됨. 정책도 계속 변함.



대표 알고리즘

: SARSA, Q-Learning



## 머신러닝 모델의 검증 및 평가



과대적합(overfitting)

: 복잡한 알고리즘을 사용하여 데이터를 훈련하는 경우 과대적합 문제를 항상 염두에 두어야 함.



모델 적합에 사용된 자료를 평가를 위해 재활용하지 않고, 평가만을 위한 데이터를 확보할 필요가 있음.

-> 훈련자료 중 일부를 떼어놓는 것. 떼어놓은 것은 훈련에 사용하지 않는다.





모델 검증 및 평가를 위한 데이터의 구분 - Hold-out 방식, K-fold 교차검증(Cross-validation) 방식



Hold-out 방식

: 주어진 자료를 다음의 세 그룹으로 랜덤하게 분할한 뒤 주어진 목적에 따라 각각 모델의 훈련, 검증, 평가에 활용한다.

- 훈련 데이터(training data) : 모델의 학습을 위해 사용되는 자료. x에 따른 y=f(x)를 찾기. 매개변수 조정
- 검증 데이터(validation data) : 훈련 데이터 중에서 또 따로 떼어서, 훈련 데이터로 적합되는 모델을 최적의 성능으로 튜닝하기 위해 사용되는 자료. 초매개변수 조정 또는 특징 변수 선택(feature variable selecting?) 등에 이용
- 평가 데이터(test data) : 훈련, 검증 데이터로 적합된 최종 모델이 미래에 주어질 새로운 데이터에 대하여 얼마나 좋은 성과를 낼지 평가하는 데 사용되는 자료



평가 데이터를 가지고 Accuracy test 등을 수행할 수 있음.

일반적으로 평가 데이터의 accuracy는 훈련 데이터의 accuracy보다 낮다

조금만 작으면 괜찮은데 이 차이가 크다면 과대적합



K-fold(다중) 교차검증(Cross-validation) 방식

: 자료의 수가 충분하지 않은 경우에는 훈련 데이터에서 너무 많은 양의 데이터를 검증, 평가 데이터에 뺏기지 않도록, 교차검정(Cross-validation) 기법을 사용한다.

보통은 5-fold, 10-fold를 많이 쓴다.

데이터를 k개 그룹으로 분할한 뒤, k-1개 그룹의 데이터로 학습, 1개 데이터로 평가하는 것을 k번 반복. 그렇게 구한 loss 또는 accuracy의 평균으로 모델을 검증, 평가하는 것이다.

계산량이 많아서 훈련하는 데 시간이 많이 걸리는 단점이 있지만, 어쨌든 가진 데이터를 한 번씩은 다 활용할 수 있다는 장점이 있다.





과대적합과 과소적합의 밸런스를 잘 맞춰야 한다.

이 밸런스를 맞출 때 알아야 하는 것

일반화 오차 및 편향-분산 트레이드 오프



편향-분산 트레이드 오프(Bias-Variance Trade off)

: 모델의 복잡도에 따라 훈련 데이터와 평가 데이터의 예측오차는 일반적으로 아래와 같은 패턴을 보이게 됨.

Prediction Error(Test Error rate)는 모델이 너무 단순해도, 모델이 너무 복잡해도 높아진다.

- 편향(bias) : 정확도 개념. 표본 데이터가 달라질 때마다 모델도 조금씩 바뀌는데 모델들의 평균이 실제 자료의 패턴을 얼마나 잘 맞추느냐를 나타낸다. 단순한 모델은 편향이 높고, 복잡한 모델은 편향이 낮다.
- 분산(variance) : 변동성 개념. 표본 데이터가 달라지면 모델이 얼마나 크게 바뀌는가를 나타낸다. 단순한 모델은 분산이 낮고(표본이 달라져도 단순한 직선 모델은 크게 달라지진 않고 기울기, 절편 살짝씩만 달라진다), 복잡한 모델은 분산이 높다(표본이 달라지면 복잡한 모델은 크게 달라진다).



일반화 오차 = 편향2 + 분산

모델이 단순하면 편향이 높아서 일반화 오차가 커지고,

모델이 복잡하면 분산이 높아서 일반화 오차가 커지는 것이다.

-> 즉, 편향도 낮으면서 분산도 낮은 지점을 찾아야 한다.



과대적합(모델 너무 복잡)을 막기 위해서는?

- 훈련 데이터를 많이 확보 (변동성 줄이기)
- 모델의 복잡도를 낮추기(단순화) : 특징 변수의 수를 줄이거나 차원축소, 파라미터에 규제 적용(Ridge, Lasso, Elasic Net 등의 방법)



## 머신러닝 모델의 평가지표

 회귀 모델의 평가 지표 - RMSE, 결정계수(R2: R-square), MAE, MAPE

RMSE(Root mean square error)

: MSE(오차 제곱의 평균)의 제곱근

실제값과 예측값의 차이(오차)의 평균

작을수록 좋다



결정계수(R\**2)

: 0~1 사이의 값

데이터 점들이 우리 모델로 얼마나 잘 요약되고 있는가를 나타냄

0이면 모델이 매우 안 좋은 상태고, 1이면 모델이 완벽히 피팅한 상태(오차가 0)

클수록 좋고 1에 가까우면 좋다



MAE(mean absolute error)

: RMSE는 제곱했다가 제곱근 취한 건데,

MAE는 그냥 절댓값을 취해서(오차의 부호를 제거) 이를 평균한 값

MAE가 10이면 오차가 평균적으로 10 정도 발생한다고 이해



MAPE(mean average percentage error)

실제 값 대비 오차가 차지하는 비중이 평균적으로 얼만지 확인



MAE, MAPE 둘 다 ‘오차의 평균’ 개념이기 때문에 작을수록 좋다



 분류 모델의 평가 지표 - 정오분류표, ROC 도표

정오분류표(교차분류표: confusion matrix)

- 정확도, 정분류율(accuracy) : 전체 중 잘 맞춘 것의 비중 = (TP + TN) / 전체
- 오분류율 : 전체 중 잘못 맞춘 것의 비중 = (FP + FN) / 전체
- 정밀도(precision) : positive로 예측한 것 중에서 실제 positive인 비중 = TP / (TP + FP). FP가 작으면 좋다. FP의 비용이 클 때 사용. ex) 스팸메일 분류
- 재현율(recall) = 민감도(sensitivity) : 실제 positive인 것 중에서 positive로 예측한 비중 = TP / (TP + FN). FN이 작으면 좋다. FN의 비용이 클 때 사용. ex) 암환자 진단

threshold(분류 기준점)가 증가하면 precision은 증가하고 recall은 감소한다.

precision과 recall은 반대로 간다.



ROC(Receiver operating characteristic) 도표

: TPR(민감도, sensitivity)과 FPR(특이도, specificity)의 조합을 도표로 나타냄

가로축이 FPR, 세로축이 TPR

FPR보다 TPR이 가파르게 증가해야 좋은 모델이다.

ROC 곡선이 초반에 급격히 상승하면 좋다.



AUC(Area Under the Curve)

: ROC 곡선 아래의 면적

1에 가까울수록 좋은 수치

FPR이 작을 때 얼마나 큰 TPR을 얻는지에 따라 결정됨



-> 어떤 관점에서 이 분류 문제를 바라보는가? 어떤 범주가 나에게 큰 비용을 발생시키는가? 어떻게 잘못 분류했을 때 큰 비용을 발생시키는가?

이에 맞게 적절한 평가 지표를 선택해야 한다. 



## 특성 공학(Feature Engineering)

: 머신러닝 알고리즘에 사용되는 입력 데이터에 해당하는 특성 변수들에 대한 처리

데이터의 양과 질에 따라 머신러닝의 성능이 달라짐

머신러닝 알고리즘에 들어가는 특성 변수들을 어떻게 선택하느냐의 문제

특성 변수들을 잘 선택해야만 좋은 예측력, 성능을 가진 머신러닝 알고리즘을 얻을 수 있다.

어떤 변수를 사전에 선별해서 넣을까?

사전에 알 수 없음.

그래서 특성 공학에서 제공하는 방법들을 사용해서 잘 선택해 보자.



특성공간 차원축소의 필요성

- 모델의 해석력 향상 : 변수가 적으면 연관 관계를 설명하기가 쉬워짐
- 모델 훈련 시간의 단축
- 차원의 저주(curse of dimensionality) 방지
- 과적합(overfitting)에 의한 일반화 오차를 줄여 성능 향상 : 변수가 많으면 억지로 끼워맞춰져서 과적합 발생. 모델에 효과적인 변수들만 고르기.



차원의 저주란??

차원이 많아져서 더 넓은 공간에 데이터를 펼쳐서 보면 데이터 사이가 넓어져(sparse) 기존의 패턴이나 연관성이 잘 보이지 않게 된다.

특성공간 차원축소를 통해 보다 더 패턴을 잘 파악할 수 있는 형태로 만들 수 있다.



특성 공학의 방법론은 크게 특성 선택 방법과 특성 추출 방법으로 구분할 수 있다.



## 특성 선택(feature selection)

Filter, Wrapper, Embedded 방식



: 전체 특성 변수 중 최적의 조합을 선택하는 문제

변형을 주는 게 아니라 일부를 선택하는 것.

가장 좋은 특성변수의 조합만 선택.

불필요한 특성 변수를 제거.



- Filter 방식

: y를 설명하는 데 중요한 순서대로 ranking을 매긴 후 순위가 높은 상위 몇 개를 선택

각 특성 변수를 독립적인 평가함수로 평가함.

특성 변수 하나씩, Xi과 Y의 1:1 관계로 연관성 판단

연관성 파악을 위해 t-test(x가 숫자형이고 y가 0 또는 1로 이진분류 문제라면 시행. p-value가 작을수록 연관성이 강한 것), chi-square test(x가 범주형이고 y가 0 또는 1로 이진분류라면 시행. 2차원 분할표를 작성하여 카이제곱 독립성 검정. p-value가 작을수록 연관성이 강함.), information gain(트리 모델에서 y가 0인 경우, 1인 경우 두 개 그룹으로 구분해서 그 그룹들의 x값들이 그룹 안에서 비슷하고 그룹 간에 이질적인가?) 등의 지표 활용



- Wrapper 방식

: 전체 특성 변수들 중 일부 set을 후보로 데려와서 모델에 피팅해서 평가해보는 것을 반복. 모델의 적합도 측면이나 어떤 평가 기준을 이용해서 최적의 조합을 판단하여 선택

k개의 특성 변수가 있다면 (2k)-1번 피팅해볼 수 있음

R-square나 RMSE 등으로 평가해보는 것

베스트 조합을 찾아낸다

모든 가능한 변수 조합을 다 살펴보는 것이라서 너무 시간과 비용이 많이 들고, k가 늘어나면 비현실적이게 된다.

순차탐색법으로 forward selection, backward selection, stepwise selection 등이 있음.



filter 방식과 wrapper 방식 비교

- filter 방식은 1:1, wrapper 방식은 다대일
- filter 방식은 모델링을 하지 않지만 wrapper 방식은 조합이 주어질 때마다 모델링(실제로 모델 학습)을 해본다.
- filter 방식은 계산 비용이 적고 속도가 빠르지만 wrapper 방식은 계산 비용 크고 속도 느림.
- filter 방식은 특성 변수들 간의 상호작용을 고려하지 않지만(1:1로 최적인 것들을 찾아 넣었는데 그것들의 조합은 최적이 아닐 수 있음), wrapper 방식은 상호작용을 고려하여 최적의 특성변수 조합을 찾아냄. 따라서 wrapper 방식에서는 특성 변수에 중복된 정보가 많은 경우 이를 효과적으로 제거할 수 있음.
- wrapper 방식에서는 과적합의 가능성이 있음.



- Embedded 방식 (좀 더 advanced된 모델을 사용하는 방식)

: 변수 선택의 기능까지 알고리즘에 포함되어 있는 모델을 이용. 다 집어넣으면 모델이 알아서 변수 선택까지 최적화를 해줌.

대표적 방법으로는 특성변수 자체에 규제를 가하는 Ridge, Lasso, Elastic Net 등이 있음.



## 특성 추출(feature extraction)

PCA(Principal component analysis), SVD(Singular value decomposition), LDA(Linear discrimination analysis), NMF(Non-negative matrix factorization)



: 단순한 선택이 아니라, 특성 변수들을 적절하게 결합, 조합, 변형하여 새로운 특성 변수를 만드는 문제. 특성을 압축하는 것



주성분 분석 (PCA: Principal component analysis)

: 서로 연관되어 있는 변수들이 관찰되었을 때(x1부터 xk가 서로 중복되는 정보들을 많이 반영하고 있을 때), 이 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수(주성분: Principal Component)를 생성하는 방법.

하나가 아니라 여러 개를 만드는데 특성 변수의 개수 k만큼, 즉 y1부터 yk까지 만들어낼 수 있다.

이 중 y1이 가장 중요하고, y2는 y1에서 설명되지 않은 정보 중에서 가장 많은 정보를 반영하는 식으로, 중요한 순대로, 정보의 양대로 내림차순 정렬된 k개의 y변수를 생성

따라서 y1으로 x들의 변동의 70%가 설명되고, y2로 10%, y3로 5%가 설명된다면, 이 y1, y2, y3 세 변수만으로 전체 k개의 x가 가지고 있던 정보의 85%가 설명되는 것이다. 3차원으로 압축! 이것이 차원 축소.

앞에서부터 소수의 PC로 원래 정보를 최대한 압축해보자는 것이 기본 아이디어.

-> 상관관계가 강한 2개의 특성 변수를 적절하게 결합해서 새로운 변수를 만드는 것!

상수를 곱하고 더하는 방식으로 새로운 축을 만든다.

고로 PCA는 결국 이 축을 어떻게 정하냐는 것인데, 어떤 축을 사용해서 새롭게 좌표값을 표현하면 가장 큰 변동이 생길까?를 기준으로 정한다.

분산이 가장 커지는 방향으로 새로운 축을 정의해서 새로운 score(새로운 축에서의 좌표값)를 매김

그 다음 y2는 y1의 정보와 독립적인 정보를 가지면서 변동성이 가장 큰 축을 정의. 즉 쉽게 말하면 y1에 수직인 축을 찾으면 된다.

직각으로 만나는 축을 만들게 됨.

변동성의 순서대로 순차적으로 y들을 매기고, 그 y들은 서로 독립적인 정보를 반영하고 있다.

첫번째 축인 y1은 전체 자료가 갖는 변동성의 많은 부분을 이 변수 하나로 설명 가능하다. 그걸로 설명 안되는 나머지가 y2로 설명됨.

따라서 2차원 상의 정보를 1차원 상의 정보로 압축이 가능한 것!!



주성분 분석의 목적

- 자료에서 변동이 큰 축을 탐색함
- 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소함
- 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 함 (직교하는 축들을 찾아내므로)



주성분 분석의 아이디어

y1 - y1의 분산이 최대가 되도록 찾음

y2 - y1에 직교하는 축을 찾되, 분산이 두 번째로 최대가 되도록 찾음

.

.

.

yk - y1부터 y(k-1)에 직교하는 축이면서, 분산이 다 반영되고 나머지의 분산이 반영됨

<- 분산최대화 기법으로 순차적으로 만들어냄.



주성분 분석에 관한 기하학적 의미

- 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석 가능
- 첫번째 주성분 축 y1은 데이터의 변동이 가장 커지는 축임
- 두번째 주성분 축 y2는 y1과 직교하며 y1 다음으로 데이터의 변동이 큰 축을 나타냄
- 각 관찰치 별 주성분 점수는 대응하는 원 자료 값들의 주성분 좌표축에서의 좌표 값에 해당함
- 주성분 분석을 이용한다는 것은 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석 가능 (직교회전을 하면 공분산이 0이 되어서 공분산 행렬이 대각행렬 형태가 됨을 의미) <- 직교회전을 하고 나면 상관관계가 0, 선형관계가 전혀 존재하지 않는, 서로 독립이 되므로



원자료의 공분산 행렬

[x1의 분산, x1, x2의 공분산,

x2, x1의 공분산, x2의 분산]

(이 때 x1, x2는 상관관계가 있기 때문에 공분산이 0이 아니다.)



직교회전 후 공분산 행렬 (대각행렬)

[y1의 분산, 0, 

0, y2의 분산]

(이 때 y1, y2는 독립이므로 둘의 공분산이 0이 된다.)





특성값 분해 (SVD: Singular value decomposition)

: 임의의 n*d 행렬 A는 어떤 세 개의 행렬의 곱 U*Σ*(VT)로 분해 가능함

이 때 U와 V는 직교행렬 (내적이 0)

Σ는 n*d의 대각행렬. r개의 람다값들이 있는데 람다1제곱근보다 람다2제곱근이 더 작아지는 방식이다.



-> U, Σ, (VT)의 각각의 열, 람다(가운데 원소), 행을 추출해서 곱해서 더하는 식으로 A를 표현 (항이 r개)



이렇게 쪼개진 행렬합이 r개인데 r개의 람다값들 중에서 의미있는 수준의 람다값 m개까지만 자를 수 있음.

쪼개진 행렬들을 m개까지만 더해도 A에 상당히 근사한 정도로 얻을 수 있음.



정보가 많은 순서대로 m개까지의 서브행렬들만 이용하여 근사하는 경우 m계수 근사라고 함.

이미지 압축에 많이 쓰임.



PCA와 SVD의 관계

\- 자료 행렬에 대한 특성값 분해로 주성분 점수를 도출 가능

SVD에서 특성값 분해를 했을 때 (VT)의 열 값들이 주성분 점수를 만들 때 x들에 곱하고 더하는 계수를 결정.

따라서 PCA를 할 때 그 PC 스코어를 계산하는 계수를 찾기 위해 SVD를 이용한다.



## Clustering

: 비지도학습 중 한 가지 알고리즘

비지도학습의 차원축소법은 데이터의 특성 변수를 압축하는 방법(열을 압축), 군집 분석은 case, 관찰치 등 사례들을 압축하는 방법(행을 압축. 변형해서 새로운 걸 마들어낸다기보다 행들을 비슷한 것들끼리 묶어서 몇 개의 덩어리로 split하는 것)



군집분석이란?

: 라벨이 없는 상태로, k개의 특성 변수들만을 이용해서 observation(관측치)들을 유사한 것들끼리 몇 개의 덩어리(군집)로 쪼개는 것. 유사하다는 것의 정의는 특성 변수의 값이 비슷한가를 기준으로 판단한다.

어떤 개체나 대상들을 밀접한 유사성(similarity) 또는 비유사성(dissimilarity)에 의하여 유사한 특성을 지닌 개체들을 몇 개의 군집으로 집단화하는 비지도학습법.

각 군집의 특성, 군집간의 차이 등에 대한 탐색 대상으로, 집단에 대한 심화된 이해가 목적.

특이 군집의 발견, 결측값의 보정(군집화해서 결측값이 포함된 군집의 평균값으로 결측값을 예측) 등에도 사용될 수 있다.



유사한 것끼리 군집화를 한다.

군집 내 동질성, 군집 외 이질성



## 계층적 군집 분석 (Hierarchical clustering)

병합적(agglomerative) vs 분할적(divisive)

- 병합적 : 개체 간 거리가 가까운 개체끼리 차례로 묶어주는 방법으로 군집을 정의
- 분할적 : 개체 간 거리가 먼 개체끼리 나누어가는 방법으로 군집을 정의



계층적 군집 분석에서는 병합적 방법이 주로 사용된다.



개체 간 거리

- 유클리디안 거리
- 맨해튼 거리
- 민코우스키 거리



군집 간 거리

- 단일연결법 (최단연결법, single linkage)

​	: 두 군집의 모든 개체 간 거리들 중 최솟값(min)

- 완전연결법 (최장연결법, complete linkage)

​	: 두 군집의 모든 개체 간 거리들 중 최댓값(max)

- 평균연결법 (average linkage)

​	: 두 군집의 모든 개체 간 거리들의 평균 = 거리들의 평균

- 중심연결법 (centroid linkage)

​	: 두 군집의 중심 사이의 거리 = 평균들의 거리

- 와드연결법 (ward linkage)

​	: SSE를 군집의 중심으로부터 해당 군집의 각 개체간의 거리 제곱 합으로 정의 -> 2개 군집을 하나의 군집으로 묶었을 때 SSE가 증가하는 정도를 두 군집 간의 거리로 정의. 즉, SSE(pooled) - (SSE1 + SSE2). 이 증가한 정도가 크면 두 군집이 멀리 떨어져 있다고 정의. 계산량이 좀 많긴 하지만 변동성을 이용해서 군집 간의 거리를 정의하는 것으로서 많이 활용되는 방법임.



개체와 군집 간 거리는 개체를 군집이라 생각하고 군집 간 거리를 적용하면 된다.





덴드로그램 (Dendrogram)

: 군집분석의 결과를 시각화한 것

군집 간 거리가 멀고, 군집 내 거리가 가까워지도록 적절한 지점에서 절단하여 군집 수를 결정한다.



계층적 군집분석의 덴드로그램 툴을 사용하면 군집의 개수를 정의하거나 군집의 형태를 묘사, 탐색하기에 도움이 되기 때문에, 최종적으로 다른 군집분석으로 군집화를 하더라도 군집의 형태를 파악하기 위해 계층접 군집분석을 보조적인 수단으로 함께 사용하는 경우가 많음.

다만 계산량이 많아서 시간이 오래 걸린다는 단점이 있다. 그럴 때는 데이터가 너무 많으면 랜덤하게 일부 뽑아서 계층적 군집분석을 돌려봐서 군집의 개수를 정의하거나 군집의 형태, 패턴 등을 먼저 탐색하는 용도로 활용하기도 한다.



덴드로그램이 주어지면 어디에서 자를지 결정

-> 군집의 수가 달라짐.



## 비계층적 군집 분석 (K-means clustering)

: 계층적 군집 분석의 대안이 될 수 있음

데이터의 용량이 크고, 빠른 계산을 원할 때 k-means clustering을 사용하면 됨



K-평균 군집분석

: 군집을 몇 개로 할지, 사전에 적절한 k를 먼저 정해야 함.

계산량이 적어서 대용량 데이터를 빠르게 처리 가능.

초기의 군집 중심이 지정되는 위치에 따라 최종 결과가 영향을 많이 받음. (가능한 한 초기군집 중심 간의 거리가 먼 것이 좋다.)

잡음이나 이상치의 영향을 많이 받음.(각 군집 중심점에 영향을 주기 때문에.)



k-평균 군집분석의 알고리즘

- 개체를 k개의 초기 군집으로 나눈다. (초기 군집에 결과가 영향을 받기 때문에 초기 군집을 바꿔서 여러 번 수행해보는 것이 바람직하다.)
- 각 군집의 중심(centroid)을 계산한 뒤 모든 개체들을 각 군집의 중심에 가장 가까운 군집에 할당시킨다.
- 새로운 개체를 받아들이거나 잃은 군집의 중심을 다시 계산한다.
- 위 과정을 더 이상의 재배치가 생기지 않을 때까지 반복한다.





k-평균 군집분석에서 적절한 군집 수 k의 결정

\- 오차제곱합(SSE: sum of squared error)

: 각 군집 내 개체들과 해당 군집 중심점과의 거리를 제곱한 값들의 합

오차제곱합이 작을수록 군집 내 유사성이 높아 잘 응집된 것.

군집 수 k에 따른 SSE의 변화를 Elbow 차트로 시각화한 뒤, SSE가 급격히 감소하다가 완만해지기 시작하는 시점의 k를 적정 군집 수로 판단한다.

꺾이는 지점이 최적의 군집 수라고 판단하는 것.

적은 수의 군집으로 작은 SSE를 달성하는 것이 목표이므로.



내가 선정한 군집 수가 적절한가, 이보다 나은 값은 없는가?



## Regression

독립변수 = 특성변수, 설명변수, 예측변수

종속변수 = 타겟변수, 반응변수



단순 회귀 : 독립변수 X가 1개

다중 회귀 : 독립변수 X가 2개 이상



일변량 회귀 : 종속변수 Y가 1개

다변량 회귀 : 종속변수 Y가 여러 개



## 단순선형회귀분석(Simple linear regression)

: 독립변수 X 하나만 가지고, 연속형(숫자형) 종속변수 Y를 예측하는 모델

선형함수



Y = α + β*X + ε



E(Y) = Y^ = α + β*X



ε 오차(관찰 불가능) = e 잔차(관찰 가능)

이 때 오차 ε는 정규, 등분산, 독립의 가정을 한다.

관찰 불가능한 오차 대신에 관찰 가능한 잔차를 사용한다.

오차에 관한 적정성 검토를 잔차를 이용하여 수행한다.



회귀계수 : 절편(α), 기울기(β)

α, β는 미지의 모수로, 상수임



우리에게 주어지는 것은 n개의 표본인 관측치(xi, yi)임.

이 n개의 점들, 즉 자료를 잘 요약하는 직선의 α^, β^을 찾아내는 것으로 모수 α, β를 추정하는 것이다.



어떻게 하면 자료를 잘 요약하는 직선인가?

최소제곱법

: n개의 ‘수직거리 제곱합’이 최소가 되도록

자료가 선과 갖는 수직 거리가 n개 있을 텐데, 그 거리가 작은 것



회귀계수의 유의성 t검정

왜 하는가?

우리가 관찰한 n개가 특이해서 우연한 값 β를 찾아낸 것이 아닌가?

과연 β가 0이 아니라고 말할 수 있는가? 하는 고민에서.



-> β에 관한 가설검정을 수행한다. (일표본에서 모수에 관한 추론 one-sample t-test와 매우 유사)

귀무가설 H0: β = 0

대립가설 H1: β != 0



귀무가설이 기각되어야만 (β가 0이 아니므로) X가 Y를 설명하기에 유용한 변수라고 할 수 있다.

-> 독립변수에 대한 유의성 검정, t-검정



귀무가설 H0이 사실일 때(β=0이면),

T라는 통계량 값(β^ / 표준오차)은 평균 0의 자유도 (n-2)의 분포



대립가설의 방향은 양쪽 꼬리 방향



p-value가 유의수준보다 작으면 귀무가설 기각

-> X가 Y를 설명하는 데 유용하다.

-> 이 회귀모델이 의미가 있다.



Y의 변동성 분해



제곱합

SST(Total: yi의 y평균으로부터의 변동) = SSR(Regression: 모형으로 설명되는 변동) + SSE(Error: 모형으로 설명되지 않는 변동)



결정계수(R\**2) = SSR/SST = 1 - SSE/SST

: 모델에 대한 적합도 지표

항상 0~1 사이 값을 가짐

yi의 총 변동 중에서, 추정된 회귀모형으로 설명되는 변동의 비중을 의미한다.

모델의 설명력을 의미!

1에 가까울수록 좋다.

두 변수 간의 상관계수 r의 제곱과 같다.

<- 선 주변의 데이터 응집력 |r|을 알 수는 있지만, 양의 상관관계인지 음의 상관관계인지는 알 수 없다.

추가로, r=1이라고 해서 기울기도 1인 것은 아니다.

r은 데이터 응집력을 의미!!



ex) R2 값이 0.65라면, Y의 총 변동 중에서 65%는 X를 이용해서 설명할 수 있다고 해석할 수 있다.



## 다중회귀분석(Multiple linear regression)

: 독립변수 X가 2개 이상

X가 1개인 단순선형회귀분석보다 적합도가 나아지고 설명력이 높아질 것으로 기대할 수 있음.



다른 변수들이 고정되어 있다는 것을 가정하고, β의 효과를 해석해야 한다.



범주형 독립변수를 회귀모형에 포함하기 위해서는 더미변수 기법을 사용해야 한다.

더미변수

: 0 또는 1의 값을 갖는 변수로 정의

더미변수의 개수 = 범주의 개수 - 1

(독립변수들 간에 완벽한 선형관계가 존재하면, 최소제곱법으로 모수(회귀계수), 즉 회귀모형 추정이 되지 않는다. 그래서 마지막 더미변수는 뺀다.)



각 범주별로 더미값 0, 1을 대입해서 회귀계수 추정식을 다시 각각 써보면, 

절편의 차이로 범주형 독립변수들에 따른 평균값 차이를 알 수 있다.



다중회귀모형의 변수선택

: 가능한 적은 수의 독립변수(설명변수)로 좋은 예측력을 갖는 모형을 찾고자 함.

적은 수의 최적의 독립변수 조합을 찾아내기!!

정보가 중복되거나, Y를 설명하는 데에 도움이 안되는 X들 걸러내고 꼭 필요한 것들만 남기자.



변수선택법 Filter, Wrapper, Embedded 가운데 Wrapper 방식

- 모든 가능한 조합의 회귀분석

​	: X가 너무 많으면 비효율적이거나 불가능

- 전진선택법 (forward selection)

​	: 절편만 있는 모델에서 출발하여 가장 중요한 변수를 하나씩 추가하는 방식

​	부분 F검정을 통해 변수의 유의성을 판단한다.

​	한번 선택된 변수는 제거되지 않는 단점이 있음 (기존에 선택된 변수와 중복된 정보를 공유하고 있는 변수가 추가될 수 있음)

- 후진제거법 (backward elimination)

​	: 모든 변수가 포함된 모델에서 가장 중요하지 않은 변수부터 하나씩 제거

​	제거하느냐 마느냐도 부분 F검정을 통해 판단

​	한번 제거된 변수는 선택되지 않는 단점이 있음

- 단계별 방법 (stepwise method)

​	: 한 번의 스텝에 전진선택법, 후진제거법을 순차적으로 하는 것.

​	선택과 제거를 매번 고민하여, 의미를 상실한 변수를 제거한다.

​	절편만 포함된 모델에서 출발해 가장 중요한 변수부터 추가하고, 모델에 포함되어 있는 변수 중에서 중요하지 않은 변수를 제거함.

​	더이상 새롭게 추가되는 변수가 없을 때까지 변수의 추가 또는 삭제를 반복.



모형 선택의 기준

: 변수 선택의 매 스텝을 진행하는 데 부분 F검정, T검정을 활용하기도 하고, 적합도 지표를 통해서 판단하기도 한다.

그러나 R2는 변수선택 고려에서는 사용하면 안 된다.

SSE는 독립변수가 늘어나면 줄어들 소지가 있다.

따라서 결정계수 R2 = SSR/SST는 새로운 독립변수가 추가되면 항상 증가한다.

(불필요한 독립변수가 추가되어도 R2가 개선된다는 것)

그러므로 R2로 판단한다면 무조건 독립변수를 많이 추가하라고 할 것이다.



이를 보완하기 위한

수정결정계수 (adjusted R\**2) = 1 - (SSE/(n-k-1)) / (SST-(n-1))

각 제곱합을 각각의 자유도로 나눈다. (k는 독립변수의 개수)

모형에 새로운 독립변수가 k만큼 추가되어도 분자, 분모가 동시에 작아진다.

자유도에 비해 SSE가 많이 줄어드는 경우, 수정결정계수가 커지는 것이다.

자유도가 패널티 역할을 하는 것이다. -> 패널티를 감안해도 SSE가 충분히 작아진다면, 수정결정계수가 증가한다고 해석하면 된다.



-> 따라서, 독립변수 선택 문제에서는 결정계수(R2)가 아니라 수정결정계수(adjusted R2)를 사용해야 가능한 한 적은 독립변수로 설명력 있는 모형을 만드는 최적의 독립변수 조합을 남길 수 있다.



이 밖에도 AIC, BIC, Mallow’s Cp 등의 다양한 적합도 지표를 이용할 수 있음.



결정계수(R2), 수정결정계수(adjusted R2)는 클수록 좋다.

AIC, BIC, Mallow’s Cp는 작을수록 좋다.

-> 이러한 적합도 지표를 통해 그 변수가 유입되는 것이 유용하다는 판단



회귀분석의 마지막 단계에서는

회귀모형에서 가정이 적절한 것인지? 체크

오차에 대한 가정(정규, 등분산, 독립)이 적절한가?를 확인하는 것이 필요

이것을 오차에 대한 추정치 개념인 잔차를 이용하여 분석한다.

이것이 잔차분석



잔차분석방법은 검정을 통한 방법과 그래프를 통한 시각적 확인 방법이 가능하다.

진단방법



시각적 방법을 이용할 경우 : 히스토그램, QQ플롯, 잔차산점도

- 오차의 정규성 확인 : 히스토그램(구간의 간격을 어떻게 설정하느냐에 그래프 모양이 영향을 많이 받음), QQ플롯(이 더 선호된다. 이론분위수와 샘플분위수를 그 표본자료를 이용해서 관찰한 다양한 p-value에 대해서 산점도 형태로 표현한 것. 직선을 기준으로 점들이 잘 모여있으면 정규분포를 따른다고 볼 수 있음. 점들의 분포가 지수함수처럼 생겼으면 오른꼬리가 긴 분포, 로그함수처럼 생겼으면 왼꼬리가 긴 분포.)
- 오차의 등분산성 확인 : 잔차산점도(가로축은 보통 y^(y추정치), xi, i 등이 올 수 있고, 세로축이 잔차 ei. x축에 따라 분산이 일정하면 등분산으로 본다.)
- 오차의 독립성 확인 : 잔차산점도(특별한 함수적인 패턴관계가 보이지 않으면 독립을 가정하는 것으로 본다.)



가정 위반이 진단되면 어떻게 해결하느냐?

해결방법

- 오차의 정규성 위반 : 변수변환(박스콕스 변환)
- 오차의 등분산성 위반 : 가중최소제곱(WLS)회귀(최소제곱합을 분산의 역수곱으로 가중치를 두는 것. 변동성이 큰 자료는 적은 비중으로 계수 추정에 반영되고, 변동성이 작은 자료는 안정적인 것이므로 가중치를 높여주는 효과를 주는 것.)
- 오차의 독립성 위반 : 시계열 분석(정상성 조건을 만족하는지, 만족한다면 autocorrelation의 구조가 어떤지 먼저 판단해야 함. -> 적절한 모델을 선택하여 분석)



다중선형회귀모형에서 하나의 Y를 여러 개의 X로 설명할 때, X들 간에는 상관관계가 어느 정도 존재하는 것은 자연스러운 현상이다. 그러나 너무 상관관계가 심해서 중복된 정보가 많으면 모형의 회귀계수 추정에 부정적 영향을 미친다.



다중공선성(multicollinearity)

: 독립변수들 간 강한 상관관계가 존재하여 회귀계수 추정에 부정적 영향을 미치는 현상

- 개별적 회귀계수 추정의 신뢰성이 떨어져 추정치를 믿을 수 없게 만듦(표본이 조금만 달라져도 회귀계수 추정량 β^의 변동성(분산)이 커지게 된다. 중복효과 때문에)
- 그런데 전반적인 모형의 적합성, 정확도는 크게 변하지 않음

결국 β는 이상하게 추정되지만, y는 안정적으로 추정이 잘 되는 현상.



다중공선성 진단 방법

VIF계수(variance inflation factor: 분산팽창요인) = 1 / (1 - Rj\**2)

Rj2 = xj를 종속변수로 두고 나머지 독립변수들로 xj를 설명하는 다중선형회귀모델에서의 결정계수

Rj2가 크다는 것은 나머지 독립변수들로 xj가 설명이 많이 된다는 뜻이다.

Rj2가 커지면 VIF계수가 커진다.

VIF계수가 5(Rj2=0.8) 또는 10(Rj2=0.9) 이상이면 다중공선성이 심각한 것으로 본다.

Rj2=0.8 이라는 것은 나머지 변수들로 xj가 80% 설명된다는 것이다.



다중공선성의 해결책

- 변수선택으로 중복된 변수를 제거
- PCA 등을 이용하여 중복된 변수를 변환하여 새로운 변수 생성
- 변수를 그대로 넣되, Rigde, Lasso 등으로 규제를 반영하여 중복된 변수의 영향력을 통제, 일부만 사용



## 규제가 있는 선형회귀모델 : Ridge, Lasso, Elastic Net

회귀계수 β, 파라미터들이 너무 커지지 않도록 규제하는 추정법

독립변수들 중 중요하지 않은 변수, 중복된 변수의 영향력을 규제하는 것

변수선택, 변수제거와 같은 효과를 기대할 수 있음

모델의 가중치를 제한하여 과적합 방지



머신러닝에서는 최소제곱합, loss function을 최소화하고자 함.

이 때 규제가 있다는 것은 길이가 긴 β^에 대해서 패널티를 두는 것. 이를 비용으로 인식하고 낮추게 됨.

즉, 수직거리 제곱합도 최소로 만들면서, 동시에 β의 값들도 전반적으로 작게 만드는 것

둘 중 무엇에 더 비중을 둘 것이냐를 결정하는 게 λ이다. λ가 크면 패널티의 비용이 높은 비중으로 인식되고, λ가 작으면 수직거리 제곱합을 최소화하는 데 보다 더 비중을 두는 것이다. λ가 0이면 패널티가 없는 것이므로 일반적인 회귀모형의 최소제곱합. λ가 크면 클수록 패널티에 대한 비중이 높아져서 β는 더욱 더 작은 값으로 추정된다.



벡터의 길이에 패널티를 두는데,

벡터의 길이를 어떻게 정의할 것인가?



- Ridge 회귀

: L2 규제

L2 norm을 패널티 항으로 정의.

βj 제곱의 합이 패널티

벡터의 길이를 인식



λ가 크면 규제 많음 -> 회귀계수 추정치가 작아짐

적절한 λ를 찾는 방법은 작은 λ에서 출발해서 시행착오를 거쳐 찾거나, 교차검증 등으로 최적화



결국 비용함수를 최소로 하는 회귀계수 β를 찾는 문제다.



Alternative formulation을 통해 λ에 대응하는 t를 두고 기하학적으로 해석을 해보면,

원점으로부터 어떤 원 안에서 찾되(제곱합이 t라는 조건 하, 원의 제약 하에), 항아리의 최솟값을 찾는 것(항아리의 등고선이 접하는 지점이 솔루션이 되는 것). -> 이러한 제약이 들어가서 0 부근의 β들이 추정치로 계산되는 것이다.



λ와 t는 1:1로 대응하는데

λ는 클수록 규제가 강한 것,

t는 작을수록 규제가 강한 것.



- Lasso 회귀

: L1 규제

L1 norm을 패널티 항으로 정의

βj 절댓값의 합이 패널티가 됨.



λ가 크면 클수록 규제가 강한 것



수직거리 제곱합 식이 절댓값 합 함수와 만나는 부분, 높은 확률로 코너, 즉 어떤 축 위에서 만날 것.

이게 일반적으로 성립하기 때문에

β1, β2, β3, β4를 구해보면 이 중 일부는 그냥 0이 된다.

<- 이게 바로 변수 선택으로 Lasso 회귀를 사용할 수 있는 이유. 자동 탈락됨.



-> Ridge는 파라미터가 0이 되지 않고 전반적으로 줄어드는 경향

Lasso는 제약 범위가 각진 형태라 파라미터 중 일부가 0이 되는 경향



최적의 지점으로 가지 못하도록 제약으로 묶어놓은 것이기 때문에

편의가 발생한다, 불편성을 만족하지 못한다고 표현한다. 오차가 생긴다는 것.

그러나 추정치의 분산이 더 작아지게 된다.

추정치가 0 부근에서만 추정되도록 한정해 놓았기 때문에.

일반화 오차는 편의2+분산인데, 분산이 줄어드는 효과가 있기 때문에 일반화 오차가 작아지기도 한다. (패널티 없는 모형에 비하여)(잘 튜닝된 λ를 쓰면)



이 아이디어는 선형회귀에만 적용되는 게 아니다.

오버피팅은 분산이 커서 생기는 문제이기 때문에, 오버피팅 문제가 있는 알고리즘에 가중치 제약을 두는 Ridge, Lasso 방식과 결합을 하면 분산을 줄여서 오버피팅 현상을 개선할 수 있다.



- Elastic Net 

: L2, L1의 규제를 혼합한 방식.

Ridge, Lasso회귀의 장점을 모두 가짐

이론적으로는 둘의 장점을 결합했지만,

추정이 복잡하기 때문에 항상 우월한 것은 아님



## Classification



## 로지스틱 회귀(Logistic Regression)

: 지도학습 중 분류 알고리즘

y가 범주형일 때 사용



이항 로지스틱 회귀 모형

: 반응변수 y의 범주가 2가지인 경우. 관심범주와 관심이 아닌 범주



Logit 함수(로그오즈)

logit(p) = log(p/(1-p))



0~1 사이의 확률값 p에 대하여,

오즈 = p / (1-p)

오즈는 0~ 무한대 사이의 실수값

p가 커지면 오즈도 커진다.

그것에 로그를 취하면 전 실구간으로 확대됨.



즉, 0~1 사이의 값 p를 전 실구간 범위로 변환한 것.



p=1/2 이면 오즈는 1이 된다.

-> 관심사건이 발생할 확률, 발생하지 않을 확률이 똑같은 상태.



파라미터 추정에 사용하는 방법

: 최대우도추정법, 경사하강법



로지스틱 회귀모형은 선형의 분리 경계면을 갖는다.

X라는 특성변수들만의 공간에서 Y를 기준짓는 선형의 경계

따라서 선형 분리가 가능한 문제만 해결할 수 있다.



시그모이드 함수는 x의 지점마다 기울기가 다 다르기 때문에

계수를 해석할 때 오즈비 개념을 활용한다.

x가 1만큼 증가하면, 성공에 대한 오즈가 exp(β)만큼 변화한다.



## 나이브 베이즈(Naive Bayes)

: 지도 학습 가운데 분류 알고리즘

나이브 베이즈 분류기는 생성 모델(generative model)임

스팸 메일 분류에 자주 활용됨



각 특성변수들이 모두 독립이라고 가정 (그래서 이름이 naive)



조건부 확률(베이즈 정리)를 이용



장점

- 데이터의 크기가 커도 연산 속도가 빠름
- 학습에 필요한 데이터 양이 적어도 좋은 성능을 보이는 편
- 다양한 텍스트 분류나 추천 등에 활용됨



단점

- Zero frequency 문제(-> 모든 빈도에 1을 더해주는 라플라스 스무딩으로 보정하여 극복)나 Underflow(컴퓨터가 계산하기에 너무 작은 숫자) 문제가 있음
- 모든 독립변수가 독립이라는 가정이 너무 단순함



## KNN(K-nearest Neigbor) 분류

: 가장 간단한 지도학습 알고리즘

훈련 데이터를 저장해두는 것이 모델을 만드는 과정의 전부임

K개의 가장 가까운 이웃 중에서 어떤 범주가 가장 비중이 높은가?



새로운 자료가 오면 가장 거리가 가까운 k개를 찾는다

k개 중 개수가 많은 라벨로 분류



K의 결정

: KNN에서 K의 결정은 매우 중요하다.

K가 작으면 이상치 등의 노이즈에 민감하게 반응 -> 오버피팅 문제 (모델 복잡, 분산 높)

K가 크면 자료의 패턴을 잘 파악할 수 없어 예측 성능이 저하됨 -> 언더피팅 문제 (모델 단순, 편향 높)

-> 하이퍼파라미터 K를 찾기 위하여 검증 데이터를 이용하여 주어진 훈련 데이터에 가장 적절한 K를 찾아야 함



거리의 측정법

- 유클리디안 거리 : 차이 제곱의 합 (물리적 거리)
- 맨해튼 거리 : 차이 절대값의 합
- 민코우스키 거리 : 차이의 절대값의 p제곱의 합의 1/p 제곱 (수학적 거리. 일반식. p=1이면 맨해튼, p=2이면 유클리디안)



거리를 측정할 때 자료의 스케일의 차이가 있는 경우,

스케일이 큰 특성변수에 의해 거리가 결정되어 버릴 수 있음.

따라서 각 특성변수별로 스케일이 유사해지도록 표준화 변환(Z score) 또는 min-max 변환으로 스케일링을 해준 뒤에 거리를 재는 것이 적절하다.



min-max 변환

: xi-min(xi) / max(xi)-min(xi)

-> min(xi)=0, max(xi)=1이 되며

크기 순서대로 정렬하여 0~1 사이 값으로 표현됨.



## Decision Tree

: 분류, 회귀에 모두 적용 가능

지도학습용 데이터가 주어졌을 때 특성변수 특징을 이용해서 자료를 분할해가는 과정

분류기준은 Y 라벨이 유사(순수)해지도록 비슷한 것끼리 서로 뭉치도록.

의사결정규칙(decision rule)을 나무 구조로 도표화하여 관심대상이 되는 집단으 로 몇 개의 소집단으로 분할하는 방식으로 분류 및 예측하는 분석 방법.

예측 과정을 눈으로 보면서 해석할 수 있다는 장점



끝마디까지 오면 끝마디에서 Y라벨의 비중이 많은 쪽으로 예측을 한다.



의사결정나무의 종류

- CART(classification and regression tree)

​	: 가장 많이 쓰임.

​	분류, 회귀 둘 다 가능.

​	분류는 지니불순도, 회귀는 MSE 또는 분산감소량을 지표로 나눔.

​	항상 이진분리

​	개별 특성변수 및 특성변수의 선형결합 형태의 분리기준도 가능

- C4.5, C5.0

​	: 엔트로피 불순도를 기준으로 나눔

​	연속형 변수는 이진분리, 범주형 변수는 (범주별) 다진분리

- CHAID

​	: 분류는 카이제곱 통계량, 회귀는 ANOVA F-통계량 사용

​	다진분리

​	변수 간 통계적 관계에 기반



- 카이제곱 통계량 : 두 범주형 변수 간 연관성 분석을 위한 검정법. 이 값이 크면 연관성이 큰 것이므로 그걸 기준으로 분리.
- ANOVA F-통계량 : 이진분리했을 때 양쪽 값들의 평균 차이가 있는가? 이 값이 크면 평균 차이가 있는 것이므로 그걸 기준으로 분리





의사결정나무의 분석절차



1. 나무의 성장(growing)

: 각 마디에서 적절한 최적의 분리규칙(splitting rule)을 찾아 나무를 성장시킴.

분리규칙은 특성변수를 기준으로, 목표변수가 분리가 잘 되도록.

하위 노드 내에서는 동질성, 하위 노드 간에는 이질성이 가장 커지도록 분류기준을 선택.

정지 규칙(stopping rule)을 만족하는 경우는 성장을 중단.



1. 가지치기(pruning)

: 아래로 내려갈수록 데이터가 줄어들어 오버피팅 가능성이 크다.

오류율(error rate)을 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지를 제거.

이때 검증 데이터를 활용하여 의미 있는 분리 규칙들만 남기기.



1. 타당성 평가

: 평가 데이터를 이용하여 의사결정나무를 평가



1. 해석 및 예측

: 구축된 나무 도형을 해석하고 분류 및 예측 모형을 설정



## 분류 나무

분류나무의 불순도

- 지니 불순도

: 1에서 각 y라벨의 비중(p)의 제곱을 모두 빼준 값 (최댓값 0.5)

- 엔트로피 불순도

: p*(log p)의 총합에 마이너스 붙인 값 (최댓값 1)

자료가 이질적일수록 값이 높아짐.



부모 마디의 불순도, 자식 마디의 불순도를 측정해서

부모 -> 자식으로 내려가면서 불순도가 얼마나 개선되었는가?

Godness of split : 불순도의 향상된 정도

가장 많이 향상될 수 있는 분리 기준을 찾자.



분류나무의 분리규칙 탐색

: 모든 특성변수와 그 특성변수들의 모든 가능한 분리점에 대해 불순도 향상도를 구한 뒤, 가장 크게 향상되는 특성변수 및 분리점을 해당 마디에서의 분리기준으로 정한다.

이것을 매 노드마다 판단.



## 회귀 나무

: Y가 숫자형



회귀나무의 분리 규칙

- 분산감소량 

: 각 그룹(자식노드) 내에서 목표변수의 분산이 작을 수록, 그룹 내 이질성이 작은 것으로 볼 수 있음. 분산감소량이 가장 커지도록 하는 분리규칙을 탐색

- ANOVA F-통계량

: 여러 그룹의 평균 차이가 있는가 비교

귀무가설 H0 : 뮤L = 뮤R

대립가설 H1 : 뮤L != 뮤R

검정통계량 F값 = 그룹간변동(SSR)/그룹내변동(SSE)

F값이 커지면 분리가 잘 된 것이다



회귀나무의 예측 방법

마지막 노드의 y값 평균을 예측값으로 한다.



decision tree의 오버피팅 방지 방법

: 지나치게 많은 마디를 갖는 decision tree는 새로운 데이터에 적용할 때 예측오차가 매우 커지는 오버피팅 상태가 되므로, 이를 방지하기 위하여 정지규칙 또는 가지치기 방법을 사용한다.



정지규칙(stopping rule)

: 나무의 성장을 멈춤.

- 모든 자료의 목표변수값이 동일할 때
- 마디에 속하는 자료의 개수가 일정 수준보다 적을 때
- 뿌리마디로부터의 깊이가 일정 수준 이상일 때
- 불순도의 감소량이 지정된 값소다 적을 때



가지치기(prunning)

: 나무를 튜닝하여 일반화 성능을 개선

- 성장이 끝난 나무의 가지를 제거하여 적당한 크기로 조절
- 적당한 크기를 결정하는 방법? 검증 데이터에 대한 예측 오류가 가장 작은 나무 모형을 찾는 것이 일반적. 이 과정은 decision tree 모형 알고리즘 내 자동화되어 있는 경우가 많음.



decision tree의 특징

장점

- 이해하기 쉬운 규칙을 생성 (if-then-else 방식)
- 특성변수 및 목표변수 둘 다 연속형, 범주형 데이터 모두 취급
- 데이터 전처리가 거의 필요하지 않음
- 이상치에 덜 민감
- 모형에 가정이 필요 없는 비모수적 모형 (모집단 분포에 대한 가정 필요없음)



단점

- 훈련결과가 불안정 (훈련 데이터 변동에 모델이 민감하여 변동성(분산)이 크다 -> 오버피팅 가능성)
- 모든 분할은 축에 수직임
- 나무가 깊어질수록 오버피팅으로 예측력이 저하되며 해석이 어려워짐



## 연관성 분석(Association rule)

: 추천 시스템 가운데 가장 널리 활용되는 연관성 분석



연관성 분석

: 연관성 규칙을 통해 하나의 거래나 사건에 포함되어 있는 둘 이상의 품목 간 상호 연관성을 발견하는 과정

고객이 동시에 구매하는 상품 간의 관계를 분석한다는 의미에서 장바구니 분석(market basket analysis)이라고도 함.



트랜잭션 데이터

: 아이템들의 집합이 영수증 개수만큼 쭉 나열되어 있는 데이터



연관규칙

if A, B, then C

{A, B} -> C

A와 B를 구매한 사람이 C도 구매하더라



연관규칙의 객관성, 의미성을 파악하기 위한 측도(지표)

- 지지도(support)

: 지지도(A->B)는 전체 구매 건수 중 A,B를 동시에 구매한 비율

P(A교B)

일단 지지도가 최소한의 일정 수준을 넘어야 연관규칙이 의미가 있다.

그 다음에 신뢰도, 향상도를 이용



- 신뢰도(confidence)

: 신뢰도(A->B)는 A를 구매한 건수 중 B도 같이 구매한 비율

조건부 확률 P(B|A) = 지지도(A->B) / 지지도(A)

이게 높으면 연관규칙이 의미있다고 볼 수 있음



- 향상도(lift)

: 향상도(A->B)는 전체에서 B를 구매한 비율에 비해, A를 구매한 고객이 B를 구매한 비율이 몇 배인가

P(B|A) / P(B) = 신뢰도(A->B) / 지지도(B)

향상도>1 이면 양의 영향력이 있음

향상도<1이면 음의 영향력이 있음

향상도=1이면 상호 연관성이 없음

-> 향상도가 1이면 서로 독립

 

연역적(apriori) 알고리즘

- 품목들의 집합별로 지지도, 신뢰도, 향상도 지표를 구해야 하는데, 품목 수가 많을 때는 연관규칙의 탐색 비용이 크게 증가함.
- 연역적 알고리즘은 더이상 탐색하지 않아도 될 품목의 조합을 찾고, 그 조합을 부분집합으로 갖는 품목의 집합들을 가지치기하여, 효율적으로 탐색하도록 함.



최소지지도 가지치기(MSP: minumum support prunning)

: 어떤 품목(집합)에 대한 지지도가 일정수준을 넘지 못하면 그 품목(집합)이 포함된 조합들은 더이상 탐색하지 않음 <- 비빈발품목


