# 특성 공학(Feature Engineering)

: 머신러닝 알고리즘에 사용되는 입력 데이터에 해당하는 특성 변수들에 대한 처리

데이터의 양과 질에 따라 머신러닝의 성능이 달라짐

머신러닝 알고리즘에 들어가는 특성 변수들을 어떻게 선택하느냐의 문제

특성 변수들을 잘 선택해야만 좋은 예측력, 성능을 가진 머신러닝 알고리즘을 얻을 수 있다.

어떤 변수를 사전에 선별해서 넣을까?

사전에 알 수 없음.

그래서 특성 공학에서 제공하는 방법들을 사용해서 잘 선택해 보자.



특성공간 차원축소의 필요성

- 모델의 해석력 향상 : 변수가 적으면 연관 관계를 설명하기가 쉬워짐
- 모델 훈련 시간의 단축
- 차원의 저주(curse of dimensionality) 방지
- 과적합(overfitting)에 의한 일반화 오차를 줄여 성능 향상 : 변수가 많으면 억지로 끼워맞춰져서 과적합 발생. 모델에 효과적인 변수들만 고르기.



차원의 저주란??

차원이 많아져서 더 넓은 공간에 데이터를 펼쳐서 보면 데이터 사이가 넓어져(sparse) 기존의 패턴이나 연관성이 잘 보이지 않게 된다.

특성공간 차원축소를 통해 보다 더 패턴을 잘 파악할 수 있는 형태로 만들 수 있다.



특성 공학의 방법론은 크게 특성 선택 방법과 특성 추출 방법으로 구분할 수 있다.



## 특성 선택(feature selection)

Filter, Wrapper, Embedded 방식



: 전체 특성 변수 중 최적의 조합을 선택하는 문제

변형을 주는 게 아니라 일부를 선택하는 것.

가장 좋은 특성변수의 조합만 선택.

불필요한 특성 변수를 제거.



- Filter 방식

: y를 설명하는 데 중요한 순서대로 ranking을 매긴 후 순위가 높은 상위 몇 개를 선택

각 특성 변수를 독립적인 평가함수로 평가함.

특성 변수 하나씩, Xi과 Y의 1:1 관계로 연관성 판단

연관성 파악을 위해 t-test(x가 숫자형이고 y가 0 또는 1로 이진분류 문제라면 시행. p-value가 작을수록 연관성이 강한 것), chi-square test(x가 범주형이고 y가 0 또는 1로 이진분류라면 시행. 2차원 분할표를 작성하여 카이제곱 독립성 검정. p-value가 작을수록 연관성이 강함.), information gain(트리 모델에서 y가 0인 경우, 1인 경우 두 개 그룹으로 구분해서 그 그룹들의 x값들이 그룹 안에서 비슷하고 그룹 간에 이질적인가?) 등의 지표 활용



- Wrapper 방식

: 전체 특성 변수들 중 일부 set을 후보로 데려와서 모델에 피팅해서 평가해보는 것을 반복. 모델의 적합도 측면이나 어떤 평가 기준을 이용해서 최적의 조합을 판단하여 선택

k개의 특성 변수가 있다면 (2k)-1번 피팅해볼 수 있음

R-square나 RMSE 등으로 평가해보는 것

베스트 조합을 찾아낸다

모든 가능한 변수 조합을 다 살펴보는 것이라서 너무 시간과 비용이 많이 들고, k가 늘어나면 비현실적이게 된다.

순차탐색법으로 forward selection, backward selection, stepwise selection 등이 있음.



filter 방식과 wrapper 방식 비교

- filter 방식은 1:1, wrapper 방식은 다대일
- filter 방식은 모델링을 하지 않지만 wrapper 방식은 조합이 주어질 때마다 모델링(실제로 모델 학습)을 해본다.
- filter 방식은 계산 비용이 적고 속도가 빠르지만 wrapper 방식은 계산 비용 크고 속도 느림.
- filter 방식은 특성 변수들 간의 상호작용을 고려하지 않지만(1:1로 최적인 것들을 찾아 넣었는데 그것들의 조합은 최적이 아닐 수 있음), wrapper 방식은 상호작용을 고려하여 최적의 특성변수 조합을 찾아냄. 따라서 wrapper 방식에서는 특성 변수에 중복된 정보가 많은 경우 이를 효과적으로 제거할 수 있음.
- wrapper 방식에서는 과적합의 가능성이 있음.



- Embedded 방식 (좀 더 advanced된 모델을 사용하는 방식)

: 변수 선택의 기능까지 알고리즘에 포함되어 있는 모델을 이용. 다 집어넣으면 모델이 알아서 변수 선택까지 최적화를 해줌.

대표적 방법으로는 특성변수 자체에 규제를 가하는 Ridge, Lasso, Elastic Net 등이 있음.



## 특성 추출(feature extraction)

PCA(Principal component analysis), SVD(Singular value decomposition), LDA(Linear discrimination analysis), NMF(Non-negative matrix factorization)



: 단순한 선택이 아니라, 특성 변수들을 적절하게 결합, 조합, 변형하여 새로운 특성 변수를 만드는 문제. 특성을 압축하는 것



주성분 분석 (PCA: Principal component analysis)

: 서로 연관되어 있는 변수들이 관찰되었을 때(x1부터 xk가 서로 중복되는 정보들을 많이 반영하고 있을 때), 이 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수(주성분: Principal Component)를 생성하는 방법.

하나가 아니라 여러 개를 만드는데 특성 변수의 개수 k만큼, 즉 y1부터 yk까지 만들어낼 수 있다.

이 중 y1이 가장 중요하고, y2는 y1에서 설명되지 않은 정보 중에서 가장 많은 정보를 반영하는 식으로, 중요한 순대로, 정보의 양대로 내림차순 정렬된 k개의 y변수를 생성

따라서 y1으로 x들의 변동의 70%가 설명되고, y2로 10%, y3로 5%가 설명된다면, 이 y1, y2, y3 세 변수만으로 전체 k개의 x가 가지고 있던 정보의 85%가 설명되는 것이다. 3차원으로 압축! 이것이 차원 축소.

앞에서부터 소수의 PC로 원래 정보를 최대한 압축해보자는 것이 기본 아이디어.

-> 상관관계가 강한 2개의 특성 변수를 적절하게 결합해서 새로운 변수를 만드는 것!

상수를 곱하고 더하는 방식으로 새로운 축을 만든다.

고로 PCA는 결국 이 축을 어떻게 정하냐는 것인데, 어떤 축을 사용해서 새롭게 좌표값을 표현하면 가장 큰 변동이 생길까?를 기준으로 정한다.

분산이 가장 커지는 방향으로 새로운 축을 정의해서 새로운 score(새로운 축에서의 좌표값)를 매김

그 다음 y2는 y1의 정보와 독립적인 정보를 가지면서 변동성이 가장 큰 축을 정의. 즉 쉽게 말하면 y1에 수직인 축을 찾으면 된다.

직각으로 만나는 축을 만들게 됨.

변동성의 순서대로 순차적으로 y들을 매기고, 그 y들은 서로 독립적인 정보를 반영하고 있다.

첫번째 축인 y1은 전체 자료가 갖는 변동성의 많은 부분을 이 변수 하나로 설명 가능하다. 그걸로 설명 안되는 나머지가 y2로 설명됨.

따라서 2차원 상의 정보를 1차원 상의 정보로 압축이 가능한 것!!



주성분 분석의 목적

- 자료에서 변동이 큰 축을 탐색함
- 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소함
- 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 함 (직교하는 축들을 찾아내므로)



주성분 분석의 아이디어

y1 - y1의 분산이 최대가 되도록 찾음

y2 - y1에 직교하는 축을 찾되, 분산이 두 번째로 최대가 되도록 찾음

.

.

.

yk - y1부터 y(k-1)에 직교하는 축이면서, 분산이 다 반영되고 나머지의 분산이 반영됨

<- 분산최대화 기법으로 순차적으로 만들어냄.



주성분 분석에 관한 기하학적 의미

- 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석 가능
- 첫번째 주성분 축 y1은 데이터의 변동이 가장 커지는 축임
- 두번째 주성분 축 y2는 y1과 직교하며 y1 다음으로 데이터의 변동이 큰 축을 나타냄
- 각 관찰치 별 주성분 점수는 대응하는 원 자료 값들의 주성분 좌표축에서의 좌표 값에 해당함
- 주성분 분석을 이용한다는 것은 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석 가능 (직교회전을 하면 공분산이 0이 되어서 공분산 행렬이 대각행렬 형태가 됨을 의미) <- 직교회전을 하고 나면 상관관계가 0, 선형관계가 전혀 존재하지 않는, 서로 독립이 되므로



원자료의 공분산 행렬

[x1의 분산, x1, x2의 공분산,

x2, x1의 공분산, x2의 분산]

(이 때 x1, x2는 상관관계가 있기 때문에 공분산이 0이 아니다.)



직교회전 후 공분산 행렬 (대각행렬)

[y1의 분산, 0, 

0, y2의 분산]

(이 때 y1, y2는 독립이므로 둘의 공분산이 0이 된다.)





특성값 분해 (SVD: Singular value decomposition)

: 임의의 n*d 행렬 A는 어떤 세 개의 행렬의 곱 U*Σ*(VT)로 분해 가능함

이 때 U와 V는 직교행렬 (내적이 0)

Σ는 n*d의 대각행렬. r개의 람다값들이 있는데 람다1제곱근보다 람다2제곱근이 더 작아지는 방식이다.



-> U, Σ, (VT)의 각각의 열, 람다(가운데 원소), 행을 추출해서 곱해서 더하는 식으로 A를 표현 (항이 r개)



이렇게 쪼개진 행렬합이 r개인데 r개의 람다값들 중에서 의미있는 수준의 람다값 m개까지만 자를 수 있음.

쪼개진 행렬들을 m개까지만 더해도 A에 상당히 근사한 정도로 얻을 수 있음.



정보가 많은 순서대로 m개까지의 서브행렬들만 이용하여 근사하는 경우 m계수 근사라고 함.

이미지 압축에 많이 쓰임.



PCA와 SVD의 관계

\- 자료 행렬에 대한 특성값 분해로 주성분 점수를 도출 가능

SVD에서 특성값 분해를 했을 때 (VT)의 열 값들이 주성분 점수를 만들 때 x들에 곱하고 더하는 계수를 결정.

따라서 PCA를 할 때 그 PC 스코어를 계산하는 계수를 찾기 위해 SVD를 이용한다.


