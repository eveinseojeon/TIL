# 4. 신경망 학습

---



## 4.2. 손실 함수(loss function)

- 오차제곱합(SSE)
  - 1/2 곱해준 것은 나중의 미분 때문에 붙여놓은 것. 전체 식을 미분하면 제곱이 앞으로 튀어나와 1/2 * 2 = 1이 되면서 제곱이 깔끔하게 사라짐!
- 교차 엔트로피 오차(CEE)
  - log0 = -inf 이므로 아주 작은 값을 더해서 절대 log0이 되지 않도록 한다.



## 4.3. 수치 미분

### 4.3.1. 미분

- 미분의 정의: '특정 순간'(한순간)의 변화량
- 즉, x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미함.
- 미분값: 접선의 기울기
- 미세한 값 h로 10<sup>-4</sup>을 이용하면 좋은 결과를 얻는다고 알려져 있다.
- (x+h)와 x의 차분(임의 두 점에서의 함수 값들의 차이)을 계산하는 것은 **전방 차분**, 전방 차분의 오차를 줄이기 위해 x를 중심으로 그 전후 (x+h)와 (x-h)의 차분을 계산하는 것은 **중심 차분** 혹은 **중앙 차분**이라 한다.
- 이처럼 아주 작은 차분으로 미분하는 것을 수치 미분이라 한다. '진정한 미분' 값을 구해주는 해석적 미분과 달리 수치 미분은 '근사치'로 계산하는 방법이다.

### 4.3.2. 편미분

- 변수가 여럿인 함수에서 '어느 변수에 대한 미분이냐'
- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다. 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다.
- 예를 들어 f(x0, x1) 함수에서 x0에 대한 편미분을 구하기 위해, 목표 변수인 x0 하나에 초점을 맞추고 다른 변수 x1은 값을 고정한다. 즉, 변수가 x0 하나뿐인 새로운 함수를 정의해서 이 함수에 대해 수치 미분 함수를 적용하여 편미분을 구한다.



## 4.4. 기울기

- 기울기: 모든 변수의 편미분을 벡터로 정리한 것
- 기울기는 각 지점에서 낮아지는 방향을 가리킨다. 더 정확히 말하면 **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향**이다!

### 4.4.1. 경사법(경사 하강법)(gradient method)

- 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값
- 기울기: 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표
- 그러므로 기울기를 잘 이용해 손실 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다.
- 경사법: 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하는 것을 반복하며 함수의 값을 점차 줄이는 것
- 기계학습을 최적화하는 데, 특히 신경망 학습에 경사법을 많이 사용한다.
- **학습률**(learning rate): 경사법 수식에서 η(에타). 갱신하는 양을 나타낸다. 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것. 신경망 학습에서는 이 학습률 값을 0.01이나 0.001 등 미리 특정 값으로 정해두고, 이 값을 변경하면서 올바르게 확습하고 있는지 확인하면서 진행한다. 이와 같은 매개변수를 초매개변수(hyper parameter)라고 한다. 사람이 직접 설정해야 하며, 일반적으로 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.

### 4.4.2. 신경망에서의 기울기

- 기울기 배열의 각 원소는 각각의 원소에 관한 편미분이다. 각 원소인 매개변수(가중치)를 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐를 나타낸다.
- 기울기 배열 dW를 구해서 보면, 기울기 배열의 형상은 가중치 배열의 형상과 같다.
- 원소 w<sub>11</sub>에 관한 편미분 값인 기울기가 0.2인 경우, 이는 w<sub>11</sub>을 h만큼 늘리면 손실함수의 값은 0.2h만큼 증가한다는 뜻이다. -> 손실 함수를 줄이기 위해서는 w<sub>11</sub>을 음의 방향으로 갱신해야 한다.
-  원소 w<sub>23</sub>에 관한 편미분 값인 기울기가 -0.5인 경우, 이는 w<sub>23</sub>을 h만큼 늘리면 손실함수의 값은 0.5h만큼 감소한다는 뜻이다. -> 손실 함수를 줄이기 위해서는 w<sub>23</sub>을 양의 방향으로 갱신해야 한다.
- 한 번에 갱신되는 양에는 w<sub>23</sub>이 w<sub>11</sub>보다 크게 기여한다.



## 4.5. 학습 알고리즘 구현하기

























데이터를 나누는 단위

- epoch
- batch
- iteration



