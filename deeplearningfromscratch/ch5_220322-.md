# 5. 오차역전파법

---



- 4장에서 신경망 학습에 대해 설명할 때, 신경망의 가중치 매개변수에 대한 손실 함수의 기울기를 수치 미분을 사용해서 구했다. 그러나 수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점이다.
- 가중치 매개변수의 기울기를 효율적으로 계산하는 **오차역전파법**(backpropagation)을 배워 보자.
- 오차역전파법을 제대로 이해하는 방법은 1) 수식을 통한 것, 2) 계산 그래프를 통한 것 두 가지가 있다. 이번 장에서는 2) <u>계산 그래프를 사용해서 '시각적'으로 이해해 보자.</u>



## 5.1. 계산 그래프

- **순전파**(forward propagation): 계산을 왼쪽에서 오른쪽으로 진행 (출발점 -> 종착점)
- **역전파**(backward propagation): 계산을 오른쪽에서 왼쪽으로 진행 (출발점 <- 종착점)
- 계산 그래프의 장점
  1. 국소적 계산에 집중한다. 즉 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있다.
  2. 중간 계산 결과를 모두 보관할 수 있다.
  3. 역전파를 통해 각 변수의 '미분'을 효율적으로 계산할 수 있다.



## 5.2. 연쇄법칙

- 역전파는 '국소적인 미분'을 오른쪽에서 왼쪽으로 전달하는 것인데, 이 전달 원리는 **연쇄법칙**(chain rule)에 따른 것이다.
- **합성 함수**: 여러 함수로 구성된 함수
- 연쇄법칙의 원리: <u>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</u>
- 역전파가 하는 일이 연쇄법칙의 원리와 같다!



## 5.3. 역전파

- 덧셈 노드의 역전파는 입력 값을 그대로 다음 노드로 흘려보낸다. (순방향 입력 신호의 값이 필요하지 않음)
- 곱셈 노드의 역전파는 입력 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 다음 노드로 흘려보낸다. (순방향 입력 신호의 값이 필요하므로 변수에 저장해 둠)



## 5.5. 활성화 함수 계층 구현하기

- ReLU 계층
- Sigmoid 계층
  - y = Sigmoid(x)일 때 Sigmoid 함수의 미분값은 y(1-y)이다.
- Affine 계층
- 





















