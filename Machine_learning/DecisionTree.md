# 의사결정나무(Decision Tree)

: 분류, 회귀에 모두 적용 가능

지도학습용 데이터가 주어졌을 때 특성변수 특징을 이용해서 자료를 분할해가는 과정

분류기준은 Y 라벨이 유사(순수)해지도록 비슷한 것끼리 서로 뭉치도록.

의사결정규칙(decision rule)을 나무 구조로 도표화하여 관심대상이 되는 집단으 로 몇 개의 소집단으로 분할하는 방식으로 분류 및 예측하는 분석 방법.

예측 과정을 눈으로 보면서 해석할 수 있다는 장점



끝마디까지 오면 끝마디에서 Y라벨의 비중이 많은 쪽으로 예측을 한다.



의사결정나무의 종류

- CART(classification and regression tree)

​	: 가장 많이 쓰임.

​	분류, 회귀 둘 다 가능.

​	분류는 지니불순도, 회귀는 MSE 또는 분산감소량을 지표로 나눔.

​	항상 이진분리

​	개별 특성변수 및 특성변수의 선형결합 형태의 분리기준도 가능

- C4.5, C5.0

​	: 엔트로피 불순도를 기준으로 나눔

​	연속형 변수는 이진분리, 범주형 변수는 (범주별) 다진분리

- CHAID

​	: 분류는 카이제곱 통계량, 회귀는 ANOVA F-통계량 사용

​	다진분리

​	변수 간 통계적 관계에 기반



- 카이제곱 통계량 : 두 범주형 변수 간 연관성 분석을 위한 검정법. 이 값이 크면 연관성이 큰 것이므로 그걸 기준으로 분리.
- ANOVA F-통계량 : 이진분리했을 때 양쪽 값들의 평균 차이가 있는가? 이 값이 크면 평균 차이가 있는 것이므로 그걸 기준으로 분리





의사결정나무의 분석절차



1. 나무의 성장(growing)

: 각 마디에서 적절한 최적의 분리규칙(splitting rule)을 찾아 나무를 성장시킴.

분리규칙은 특성변수를 기준으로, 목표변수가 분리가 잘 되도록.

하위 노드 내에서는 동질성, 하위 노드 간에는 이질성이 가장 커지도록 분류기준을 선택.

정지 규칙(stopping rule)을 만족하는 경우는 성장을 중단.



1. 가지치기(pruning)

: 아래로 내려갈수록 데이터가 줄어들어 오버피팅 가능성이 크다.

오류율(error rate)을 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지를 제거.

이때 검증 데이터를 활용하여 의미 있는 분리 규칙들만 남기기.



1. 타당성 평가

: 평가 데이터를 이용하여 의사결정나무를 평가



1. 해석 및 예측

: 구축된 나무 도형을 해석하고 분류 및 예측 모형을 설정



## 분류 나무(Classification Tree)

분류나무의 불순도

- 지니 불순도

: 1에서 각 y라벨의 비중(p)의 제곱을 모두 빼준 값 (최댓값 0.5)

- 엔트로피 불순도

: p*(log p)의 총합에 마이너스 붙인 값 (최댓값 1)

자료가 이질적일수록 값이 높아짐.



부모 마디의 불순도, 자식 마디의 불순도를 측정해서

부모 -> 자식으로 내려가면서 불순도가 얼마나 개선되었는가?

Godness of split : 불순도의 향상된 정도

가장 많이 향상될 수 있는 분리 기준을 찾자.



분류나무의 분리규칙 탐색

: 모든 특성변수와 그 특성변수들의 모든 가능한 분리점에 대해 불순도 향상도를 구한 뒤, 가장 크게 향상되는 특성변수 및 분리점을 해당 마디에서의 분리기준으로 정한다.

이것을 매 노드마다 판단.



## 회귀 나무(Regression Tree)

: Y가 숫자형



회귀나무의 분리 규칙

- 분산감소량 

: 각 그룹(자식노드) 내에서 목표변수의 분산이 작을 수록, 그룹 내 이질성이 작은 것으로 볼 수 있음. 분산감소량이 가장 커지도록 하는 분리규칙을 탐색

- ANOVA F-통계량

: 여러 그룹의 평균 차이가 있는가 비교

귀무가설 H0 : 뮤L = 뮤R

대립가설 H1 : 뮤L != 뮤R

검정통계량 F값 = 그룹간변동(SSR)/그룹내변동(SSE)

F값이 커지면 분리가 잘 된 것이다



회귀나무의 예측 방법

마지막 노드의 y값 평균을 예측값으로 한다.



decision tree의 오버피팅 방지 방법

: 지나치게 많은 마디를 갖는 decision tree는 새로운 데이터에 적용할 때 예측오차가 매우 커지는 오버피팅 상태가 되므로, 이를 방지하기 위하여 정지규칙 또는 가지치기 방법을 사용한다.



정지규칙(stopping rule)

: 나무의 성장을 멈춤.

- 모든 자료의 목표변수값이 동일할 때
- 마디에 속하는 자료의 개수가 일정 수준보다 적을 때
- 뿌리마디로부터의 깊이가 일정 수준 이상일 때
- 불순도의 감소량이 지정된 값소다 적을 때



가지치기(prunning)

: 나무를 튜닝하여 일반화 성능을 개선

- 성장이 끝난 나무의 가지를 제거하여 적당한 크기로 조절
- 적당한 크기를 결정하는 방법? 검증 데이터에 대한 예측 오류가 가장 작은 나무 모형을 찾는 것이 일반적. 이 과정은 decision tree 모형 알고리즘 내 자동화되어 있는 경우가 많음.



decision tree의 특징

장점

- 이해하기 쉬운 규칙을 생성 (if-then-else 방식)
- 특성변수 및 목표변수 둘 다 연속형, 범주형 데이터 모두 취급
- 데이터 전처리가 거의 필요하지 않음
- 이상치에 덜 민감
- 모형에 가정이 필요 없는 비모수적 모형 (모집단 분포에 대한 가정 필요없음)



단점

- 훈련결과가 불안정 (훈련 데이터 변동에 모델이 민감하여 변동성(분산)이 크다 -> 오버피팅 가능성)
- 모든 분할은 축에 수직임
- 나무가 깊어질수록 오버피팅으로 예측력이 저하되며 해석이 어려워짐


