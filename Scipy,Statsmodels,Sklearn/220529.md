## 모평균 비교에 관한 가설검정: t-test

- 단일 표본 t검정

  : 단일 모집단에서 추출된 하나의 표본집단이 대상. 모평균과 표본평균의 차이를 검정

  [scipy] **ttest_1samp()** - 모집단의 평균은 popmean 인자에 지정

- 대응 표본 t검정

  : 동일한 모집단으로부터 추출된 두 표본집단이 대상. 표본이 정규성을 만족하지 못하는 경우 Wilcoxon rank sum test 사용

  [scipy] **ttest_rel()** - 검정을 실시하는 두 변수를 차례대로 지정

- 독립 2표본 t검정

  : 보통 t-test라고 하면 이것으로, 비중이 꽤나 높음. 독립된 두 표본집단이 대상. 등분산 여부에 따라 검정통계량 계산식이 다름. 표본이 정규성을 만족하지 못하는 경우 Wilcoxon rank sum test 사용

  [scipy] **ttest_ind()** - 검정을 실시하는 두 변수를 차례대로 지정. 등분산 가정을 만족하는 경우 equal_var 인자에 True를 할당

---

## 모평균 비교에 관한 가설검정: One way ANOVA

- 일원 분산 분석(One way ANOVA)

  : scipy보다 statsmodels 사용 추천. 셋 이상의 여러 집단의 평균을 비교하는 것. 수치형 종속변수와 명목형 독립변수가 각각 1개일 때 실시하는 분석. 종속변수로 나뉘어지는 그룹이 2개 이상일 때 사용. 귀무가설을 기각하는 경우 사후 검정(Post-hoc) 실시. 사후 검정 방법으로는 Tukey's HSD, Duncan's MRT, Scheffe's test가 있음. 이는 모든 집단간 독립 2표본 t검정을 하는 것과 유사하며 어떤 집단간 평균이 유의하게 차이나는지 알 수 있음

[scipy] **f_oneway()** - 각각의 집단을 pd.Series로 입력

[statsmodels] **ols()** - 모델을 생성하고 적합. '종속변수~독립변수' 수식 입력 시 독립변수에 C() 함수 사용

[statsmodels] **anova_lm()** - 적합된 모델 정보를 기반으로 일원 분산 분석표를 보여줌

[statsmodels] **pairwise_tukeyhsd()** - 함수 내에 종속변수와 독립변수를 차례대로 선언. 결과 출력을 위해 반드시 print() 함수 사용. reject 변수의 True는 귀무가설을 기각한다는 의미

---

## 모평균 비교에 관한 가설검정: Two way ANOVA

- 이원 분산 분석(Two way ANOVA)

  : 수치형 종속변수 1개, 명목형 독립변수 2개일 때 실시하는 분석. 두 요인이 상호간 영향을 주고받으면서 나타나는 교호작용 효과(interaction effect) 확인 가능

  - 가설 - 주요 효과(main effect)

    H0(귀무가설): 집단간 평균이 같음

    H1(대립가설): 평균이 같지 않은 집단이 한 쌍 이상 존재

  - 가설 - 교호작용 효과(interaction effect)

    H0(귀무가설): 요인간 교호작용이 없음

    H1(대립가설): 요인간 교호작용이 있음

[statsmodels] **ols()** - 분산 분석을 수행. 이원 분산 분석에서는 각 요인의 교호작용을 확인하기 위해 ':' 사용

---

## 모분산 비교에 관한 가설검정: 등분산 검정(F-test of equality of variances)

- F-test

  : 두 집단의 등분산 검정을 실시. 각 집단이 정규분포를 따를 때

  [scipy] **f.cdf()** - F 검정통계량을 입력받아 p-value를 산출하는 함수. F 검정통계량, 첫번째 데이터의 자유도(dfd: len()-1), 두번째 데이터의 자유도(dfn: len())가 입력으로 필요

- Bartlett's test

  : 두 집단 이상의 등분산 검정을 실시. 각 집단이 정규분포를 따를 때 -> ANOVA와 함께 쓰임

  [scipy] **bartlett()**

- Levene's test

  : 두 집단 이상의 등분산 검정을 실시. 각 집단이 정규분포를 따를 필요 X

  [scipy] **levene()**

---

## 범주형 변수 간의 독립성 검정(Chi-squared test)

두 명목형 변수를 대상으로 실시하는 분석

독립 관점에서의 해석과 연관 관점에서의 해석이 존재한다. 귀무가설 채택(두 변수는 서로 독립이며 연관X) vs 귀무가설 기각(두 변수는 서로 독립이 아니며 연관 O)

연속형 변수의 경우 명목형 변수로 변환 후 실시

[scipy] **chi2_contingency()** - 두 개의 명목형 변수의 각 원소의 빈도를 입력한다(crosstab() 사용). 출력은 검정통계량, p-value, 자유도, 기대도수 이렇게 4개의 연산 결과가 튜플로 산출된다. correction 인자에 False 넣으면 연속성 수정을 적용하지 않음

---

## 시계열 분석

- 평활화(Smoothing)

  : 시계열 데이터를 일련의 정제법을 사용하여 보다 부드럽게 만드는 과정

  : 이동평균법은 단순이동평균법(Simple Moving Average), 가중이동평균법(Weighted Moving Average)이 있음

  : 지수평활법은 단순/이중/삼중 지수평활법이 있으며 각각 EWMA, Winters, HoltWinters로 불림

  [pandas] **rolling()** - 단순이동(평균)을 수행하는 메서드. window 인자에는 이동평균 대상이 되는 데이터 개수를 지정(n을 입력하면 (n-1)개의 결측치 생김). 뒤에 붙이는 메서드에 따라서 각 구간의 연산 결과가 달라짐(단순이동까지만 수행하므로 뒤에 mean()까지 불여줘야 단순이동평균이 됨). center 인자에 True를 입력할 경우 중심이동평균 실시 가능

  [pandas] **ewm()** - 지수가중이동(평균)을 수행하는 메서드. alpha 인자에는 지수평활계수 입력. 뒤에 붙이는 메서드에 따라서 각 구간의 연산 결과가 달라짐(단순이동까지만 수행하므로 뒤에 mean()까지 불여줘야 지수가중이동평균이 됨)

- 시계열 분해(Time Series Decomposition)

  : 시계열 데이터를 일련의 공식을 활용하여 추세, 변동 등 세부 시계열 요소로 분리하는 일련의 과정

  : 고전 시계열 분해법은 가법모형(Addictive Model)과 승법모형(Multiplicative Model)이 있음

  [statsmodels] **seasonal_decompose()** - 시계열 분해를 위한 함수. model 인자에 'multiplicative'를 입력하면 승법모형 적용(기본은 가법모형). 입력하는 시계열 데이터는 pandas의 시리즈이며 인덱스는 시간 데이터 필수. 결과를 변수로 받아서 plot()해야 보임. 결과로 받은 변수에 온점(.) 찍고 tab키 누르면 어트리뷰트(residual, seasonal, trend 등) 확인 가능

---

## 상관분석

두 변수의 선형관계를 확인하기 위해 실시

상관계수가 0에 가까울수록 선형관계가 약하며, 절댓값이 1에 가까울수록 선형관계가 강함

|                                   | Quantitiative  | Ordinal                | Nominal          |
| --------------------------------- | -------------- | ---------------------- | ---------------- |
| **Quantitiative(수치형, 연속형)** | **Pearson's**  | Biserial               | Point Biserial   |
| **Ordinal(순서형)**               | Biserial       | **Spearman / Kendall** | Rank Biserial    |
| **Nominal(명목형)**               | Point Biserial | Rank Biserial          | Phi, L, C Lambda |

[pandas] **corr()** - 상관계수를 계산하는 데이터프레임 전용 메서드. method 인자에 'pearson', 'spearman',  'kendall'

[scipy] **pearsonr()** - Pearson 상관분석을 실시. 두 일차원 벡터를 입력으로 넣고, 상관계수와 p-value가 차례대로 출력

[scipy] **spearmanr()** - Spearman 상관분석을 실시. 두 일차원 벡터를 입력으로 넣고, 상관계수와 p-value가 차례대로 출력

[scipy] **kendalltau(**) - Kendall 상관분석을 실시. 두 일차원 벡터를 입력으로 넣고, 상관계수와 p-value가 차례대로 출력

---

## 계층적 군집분석(Hierarchical Clustering)

데이터간 유사도를 기반으로 계산하며 군집의 개수가 정해져 있지 않음

계층적 군집분석을 실시하는 과정과 실시 후 특정 군집 개수로 데이터를 라벨링하는 과정이 있음

데이터의 변동에 민감하며 학습 데이터가 많을 경우 연산에 많은 시간 소요(데이터 5000~10000개 이상은 권장하지 않음)

- 계층도(Dendrogram)

  : 계층적 군집분석의 산출물 중 하나로, 데이터간 거리 기반으로 도식화한 도표

  : 계층도의 높이는 데이터 또는 군집간 거리에 비례

[sklearn] **AgglomerativeClustering()** - 계층적 군집분석을 실시할 수 있는 함수. n_clusters 인자에 분리할 군집 개수를 설정. affinity 인자에 데이터간 거리 계산 방법('euclidean' 등). linkage 인자에 군집 간 유사도 계산 방법 설정('single', 'ward' 등). 모델 객체를 받아서 어트리뷰트 labels_를 보면 군집 라벨링되어있음.

[scipy] **linkage()** - 데이터간 거리 계산 및 군집 형성을 실시. method 인자에 군집 간 유사도 계산 방법 설정('single', 'ward' 등). metic 인자에 데이터간 거리 계산 방법('euclidean' 등).

[scipy] **dendrogram()** - linkage() 함수의 결과를 받아서 계층도 시각화

---

## 비계층적 군집분석(K-means Clustering)

임의의 k개의 점을 기반으로 가까운 거리의 데이터를 묶는 것과 더불어 평균을 활용하는 군집분석 기법

군집 개수(k)를 확정하기 위해 여러 번의 시행착오 필요

결과 고정을 위해 seed 설정 필요

[sklearn] **KMeans()** - k-means 군집분석을 실시하는 함수. n_clusters, max_iter, random_state 인자에 각각 군집 개수, 최대 반복 연산, 결과 고정 설정 가능. KMeans() 함수의 fit() 메서드에 데이터를 할당하여 학습 진행. 결과 객체의 cluster_centers_와 labels\_ 어트리뷰트로 군집 중심과 각 행의 군집 번호 확인 가능

- 정규화 및 표준화

  [sklearn] **MinMaxScaler()** - MinMax 정규화를 실시하는 함수. fit() 메서드로 규칙 모델 만들고 transform() 함수로 변환 실시

  [sklearn] **StandardScaler()** - 표준화를 실시하는 함수. fit() 메서드로 규칙 모델 만들고 transform() 함수로 변환 실시

---

## 단순 회귀분석(Simple Linear Regression)

연속형 종속변수와 독립변수 간 선형관계 및 설명력을 확인하는 기법

설명력과 오차평가지표로 모델의 성능을 평가

[statsmodels] **ols()** - 선형회귀분석을 위한 함수. ols() 함수 내에 종속변수와 독립변수를 선언. ols() 함수의 fit() 메서드로 모델 적합. 변수명에 온점(.) 등 특정 특수문자가 있는 경우 오류 발생. 모델 객체의 predict() 메서드로 예측(학습하지 않은 변수들도 함께 넣어도 편리하게 알아서 걸러준다). model.summary() 해서  F검정통계량에 대한 p-value, 즉 모델의 유의성을 검정한 값인 Prob(F-statistic)을 봤을 때 0.05보다 커서 귀무가설 기각 못하면 독립변수와 종속변수 간 선형관계가 만족되지 않는다는 것. 그럼 모델 버려야 함. 0.05보다 작으면 귀무가설 기각, 즉 선형관계를 만족한다는 것. 그 다음 R-squared(결정계수)와 Adj.R-squared(수정된/조정된 결정계수)가 1에 가까울수록 설명력이 좋은 모델. 또 독립변수의 t검정통계량에 대한 p-value P>|t| 부분을 확인했을 때 0.05보다 작으면 귀무가설을 기각하여 해당 변수의 coef(계수) 값이 통계적으로 유의하다는 뜻(0이 아님). 

<u>sklearn은 머신러닝 전문 라이브러리!</u>

[sklearn] **LinearRegression()** - 선형회귀분석을 위한 함수(ols()보다 강력한 기능을 가짐). LinearRegression() 함수 내 fit_intercept로 절편 적합 여부 설정 가능. LinearRegression() 함수의 fit() 메서드에 학습 데이터 할당 가능(이때 X 인자에 2차원 데이터프레임을 넣어줘야 함). 모델 객체의 coef_, intercept\_ 어트리뷰트로 각각 계수와 절편 확인 가능. 모델 객체의 predict() 메서드로 예측(이때도 predict() 안에 2차원 데이터프레임을 넣어줘야 함)

[sklearn] **mean_absolute_error()** - MAE 연산(모델 평가 함수)

[sklearn] **mean_squared_error()** - MSE 연산(모델 평가 함수). 해당 결과를 제곱하면 RMSE(Root Mearn Squared Error)

---

## 다중 회귀분석(Multiple Linear Regression)

연속형 종속변수와 두 개 이상의 독립변수 간 선형관계 및 설명력을 확인하는 기법

필요 시 모델 성능 향상을 위한 파생변수 생성 및 성능 비교 필요

명목형 변수가 독립변수인 경우 가변수 변환 후 모델 적합

* 다중공선성 문제

  : 독립변수 간 강한 상관관계가 나타나는 문제. 상관계수를 확인하여 그 값이 높은 것을 사전에 제거. 회귀 모델 생성 이후 분산팽창계수(VIF) 확인하여 관련 변수(10 이상) 처리

  [patsy] **dmatrices()** - 수식을 기반으로 데이터 행렬을 생성. y, X로 결과를 받음. 분산팽창계수 확인을 위해 입력 데이터를 전처리할 때 필요한 함수. return_type 인자에 'dataframe'으로 설정하면 후처리 용이 <- 데이터프레임에 corr() 함수 써서 상관계수 높은(다중공선성 문제가 있는) 변수들을 확인할 수도 있음

  [statsmodels] **variance_inflation_factor()** - 분산팽창계수를 연산하기 위한 함수. 반복문 또는 list comprehension 사용

---

## 분류: 로지스틱 회귀분석(Logistic Regression)

이항 로지스틱 회귀 분석은 종속변수가 0과 1이며 베르누이 분포를 따를 경우 사용

모델의 산출 값은 각 데이터가 1이 될 확률이며 이진 분류를 위해 경계값(threshold)이 필요

모델 평가를 위해 각종 분류 관련 지표 및 AUC 활용

* 승산비(OR, Odds Ratio)

  : 특정 독립변수를 제외한 나머지 값을 고정하고 해당 독립변수가 1 증가 시 변화하는 승산(odds)의 비

[statsmodels] **Logit()** - 로지스틱 회귀분석을 실시하는 함수. endog, exog 인자에 각각 종속변수와 독립변수를 할당, 산출 모델 객체의 params 어트리뷰트에 모델의 계수 저장, 산출 모델 객체의 predict() 메서드로 예측값을 생산하며 이는 종속변수가 1이 될 확률값

[sklearn] **LogisticRegression()** - 로지스틱 회귀분석을 실시하는 함수. fit_intercept, solver 인자로 절편 적합 여부, 최적화 알고리즘 설정 가능. random_state 인자에 자연수를 할당하여 결과 고정 가능. fit() 메서드에 독립변수 및 종속변수 할당. 산출 모델 객체의 coef_ 어트리뷰트에 모델의 계수 저장. 산출 모델 객체의 predict_proba() 메서드로 예측값을 생산하며 두 번째 열이 종속변수가 1이 될 확률값

[sklearn] **roc_auc_score()** - AUC(Area Under Curve)를 산출하는 함수. y_true, y_score 인자에 각각 종속변수와 예측 확률값을 할당

[sklearn] **accuracy_score()** - 분류 모델의 정확도를 산출하는 함수. y_pred와 y_true에 각각 예측 분류 결과와 실제 값을 할당

[sklearn] **f1_score()** - 분류 모델의 f1 값을 산출하는 함수. y_pred와 y_true에 각각 예측 분류 결과와 실제 값을 할당

[sklearn] **precision_score()** - 분류 모델의 정밀도(precision)를 산출하는 함수. y_pred와 y_true에 각각 예측 분류 결과와 실제 값을 할당

[sklearn] **recall_score()** - 분류 모델의 재현율(recall)을 산출하는 함수. y_pred와 y_true에 각각 예측 분류 결과와 실제 값을 할당

---

## 분류: 나이브 베이즈(Naïve Bayes)

- 나이브 베이즈 분류기의 특징

  : 사전 확률 및 추가 정보를 기반으로 사후 확률을 추론하는 통계적 방법인 베이즈 추정 기반의 분류

  : 종속변수 각 범주의 등장 빈도인 사전확률(prior) 설정이 중요

  : 각 데이터의 사전 확률을 기반으로 사후확률(posterior)을 계산

[sklearn] **GaussianNB()** - 나이브 베이즈 분류 모델을 위한 함수. 독립변수와 종속변수는 GaussianNB() 함수의 메서드인 fit() 함수에 할당. 모델 객체의 predict_proba() 메서드로 예측 확률값을 생산. 이진 분류의 경우 출력된 예측 확률값의 두 번째 열이 1이 될 확률(threshold 정해야 함)

## KNN(K-Nearest Neighbor)

- KNN 분류(Classification)

  : 새로운 값은 기존의 데이터를 기준으로 가장 가까운 k개의 최근접 값을 기준으로 분류됨

  : k는 동률의 문제 때문에 짝수는 되도록 피하는 것이 좋음

  : k가 1에 가까울수록 과적합, k가 클수록 과소적합이 되기 때문에 적절한 k값 선정 필요

  [sklearn] **KNeighborsClassifier()** - KNN 분류 모델을 학습하기 위한 함수. n_neighbors 인자에 학습 시 고려할 이웃 데이터의 개수를 지정. n_neighbors가 1에 가까울수록 과적합되며 커질수록 과소적합되는 경향 존재. KNeighborsClassifier() 함수의 fit() 메서드에 독립변수와 종속변수 할당

- KNN 회귀(Regression)

  : 기본 개념은 분류 모델과 같으며, k개의 인접한 자료의 (가중)평균으로 예측

  [sklearn] **KNeighborsRegressor()** - KNN 회귀 모델을 학습하기 위한 함수. n_neighbors 인자에 학습 시 고려할 이웃 데이터의 개수를 지정. n_neighbors가 1에 가까울수록 과적합되며 커질수록 과소적합되는 경향 존재. KNeighborsRegressor() 함수의 fit() 메서드에 독립변수와 종속변수 할당

---

## 의사결정나무 모델: 분류 및 회귀나무

* 분류 나무(Classification Tree)

  : 종속변수가 명목형인 경우 사용하는 의사결정나무 모델. 각 노드 분류 알고리즘은 이진 분류 시 지니지수(Gini index) 기반의 CART 사용. 과적합 방지 및 모델 단순화를 위해 Depth, impurity 등 관련 설정 활용

  [sklearn] **DecisionTreeClassifier()** - max_depth, random_state로 모델의 성장과 결과 고정 설정 가능. 함수의 fit() 메서드에 독립변수와 종속변수를 할당

* 회귀 나무(Regression Tree)

  : 종속변수가 연속형인 경우 사용하는 의사결정나무 모델. 각 노드 분류에는 평균과 표준편차를 활용. 과적합 방지 및 모델 단순화를 위해 Depth, impurity 등 관련 설정 활용

  [sklearn] **DecisionTreeRegressor()** - max_depth, random_state로 모델의 성장과 결과 고정 설정 가능. 함수의 fit() 메서드에 독립변수와 종속변수를 할당

---

## 추천: 연관성 분석(Association Rule)

: 상품 또는 서비스 간의 관계 속에서 유용한 규칙을 찾을 때 사용

: 유통 분야에서 주로 활용되며 장바구니 분석(Marker Basket Analysis)이라는 별칭 존재

: 비즈니스적으로 중요한 요소를 고려하기 어렵고 연산량이 많음

- 주요 평가 지표
  - 지지도(Support): 상품 X와 Y를 동시에 구매한 비율. 규칙의 중요성
  - 신뢰도(Confidence): 상품 X를 구매 시 Y를 구매한 비율(조건부확률). 규칙의 신뢰성
  - 향상도(Lift): 상품 X 구매 시 임의 상품 구입 대비 Y를 포함하는 경우의 비중. 규칙의 상관성
- 향상도 해석
  - Lift > 1 : 품목 간 양의 상관관계(보완재)
  - Lift = 1 : 품목 간 상호독립관계
  - Lift < 1 : 품목 간 음의 상관관계(대체재)

[mlxtend] **apriori()** - 구매 아이템 빈도를 계산하는 함수. 입력 데이터 세트는 구매 아이템 기반으로 더미변수화(OHE)되어 있어야 함(pivot_table() 함수로 전처리). min_support, max_len 인자로 최소 지지도, 아이템 조합 최댓값을 설정. use_colnames 인자를 꼭 True로 하여 분석

[mlxtend] **association_rules()** - 구매 아이템 빈도를 활용하여 연관규칙을 계산하는 함수. apriori()의 결과를 입력받음. metric 인자에 필터링 기준 지표를 설정하고 min_threshold 인자에 그 경계값을 지정

---

## 주성분 분석(PCA)

- 주성분(Principal Component)

  : 입력 변수를 기반으로 최대의 분산을 가지는 새로운 변수

  : 각 주성분은 직교하기 때문에 상관계수가 0에 가까움

- 주성분 분석(PCA)의 특징

  : 특정 데이터의 주성분(PC)을 찾는 방법

  : 대표적인 차원 축소 기법

  : 입력 변수 개수와 각 주성분의 설명 비를 고려하여 주성분 개수 결정

[sklearn] **PCA()** - 주성분 분석을 시행하는 함수. n_component 인자에 산출할 주성분 개수 입력. PCA() 함수로 생성한 객체의 fit_transform() 메서드로 주성분 연산. PCA() 함수로 생성한 객체의 explained_variance로 각 주성분의 분산 파악 가능(explained_variance_ratio를 많이 씀). singular_values로 각 주성분의 고유값 확인 가능

[pandas] **cumsum()** - 숫자 원소가 있는 시리즈 객체의 누적합을 계산하기 위한 메서드. 주성분의 분산 또는 분산비를 활용하여 누적 분산 또는 누적 분산비 계산 용이

